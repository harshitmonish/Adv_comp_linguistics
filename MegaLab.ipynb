{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MegaLab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Defining Functions for Data Loading"
      ],
      "metadata": {
        "id": "sMNCKEpTQpHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y68HddTxQgFI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Use this file as a starting point to understand how to load in the data.\n",
        "\"\"\"\n",
        "\n",
        "import click\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "TRAIN_PATH = './data/train.jsonl'\n",
        "TEST_PATH = './data/test.jsonl'\n",
        "VECS_PATH = './data/buckeye.vecs'\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_DATA = True\n",
        "\n",
        "\n",
        "\n",
        "def read_jsonl_file(path: str):\n",
        "    data = []\n",
        "    with open(path, 'r') as fid:\n",
        "        for line in fid:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "def compute_log_duration(record):\n",
        "    return np.log(\n",
        "        np.sum(record['segment_duration_ms']))\n",
        "\n",
        "\n",
        "def extract_phones(record):\n",
        "    return record['observed_pron'].split(\" \")\n",
        "\n",
        "\n",
        "def load_vecs(path):\n",
        "    with open(path, 'rb') as fid:\n",
        "        return pickle.load(fid)\n",
        "\n",
        "\n",
        "def get_embedding(record, vecs):\n",
        "    return vecs[record['word']]\n",
        "\n",
        "\n",
        "def read_record(record, vecs):\n",
        "    phones = extract_phones(record)\n",
        "    log_duration = compute_log_duration(record)\n",
        "    embedding = get_embedding(record, vecs)\n",
        "    return phones, embedding, log_duration\n",
        "\n",
        "\n",
        "class BuckeyeDataset(Dataset):\n",
        "    def __init__(self, jsonl_path, embeddings_path):\n",
        "        self.records = read_jsonl_file(jsonl_path)\n",
        "        self.vecs = load_vecs(embeddings_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.records[idx]\n",
        "        phones, embedding, log_duration = read_record(record, self.vecs)\n",
        "        return phones, embedding, log_duration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear model"
      ],
      "metadata": {
        "id": "ejJhIB11c3Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from train_test_models import BuckeyeDataset, DataLoader\n",
        "\n",
        "INPUT_SIZE = 1          # number of segments\n",
        "OUTPUT_SIZE = 1         # total word duration\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0\n",
        "MOMENTUM = 0.001\n",
        "MAX_LOSS = 1.\n",
        "N_EPOCHS = 20000\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "EVERY = 2000\n",
        "VECS_PATH = './data/buckeye.vecs'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = \"cpu\"\n",
        "\n",
        "training_data = BuckeyeDataset(\"./data/train.jsonl\", VECS_PATH)\n",
        "test_data = BuckeyeDataset(\"./data/test.jsonl\", VECS_PATH)\n",
        "train_dataloader = DataLoader(\n",
        "    training_data, batch_size=64, shuffle=SHUFFLE, collate_fn=lambda x: x\n",
        ")\n",
        "\n",
        "class LinearModel(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "        \n",
        "####################################################\n",
        "### MODEL WITH JUST LENGTH IN NUMBER OF SEGMENTS ###\n",
        "model = LinearModel(INPUT_SIZE, OUTPUT_SIZE).to(device)\n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "    criterion = torch.nn.MSELoss() \n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "    # make sure we \"zero out\" the loss at each time step\n",
        "    optimizer.zero_grad()\n",
        "    batch_data = next(iter(train_dataloader))\n",
        "    xs, ys = [], []\n",
        "    for segments, embedding, duration in batch_data:\n",
        "        xs.append(len(segments))\n",
        "        ys.append(duration)\n",
        "    xs = torch.Tensor(xs).reshape(-1, 1).to(device)\n",
        "    ys = torch.Tensor(ys).reshape(-1, 1).to(device)\n",
        "    loss = criterion(model(xs), ys)\n",
        "    if i % EVERY==0:\n",
        "        print(f\"Loss: {loss}\")\n",
        "    # do backprop over that loss\n",
        "    loss.backward()\n",
        "    b, m = model.parameters()\n",
        "    #if i % EVERY==0:\n",
        "    #    print(f\"Intercept: {b.detach()[0]}, Slope: {m.detach()[0]}\")\n",
        "    # move on to the next time step\n",
        "    optimizer.step()\n",
        "\n",
        "## TEST ##\n",
        "test_criterion = torch.nn.MSELoss()\n",
        "test_xs, test_ys = [], []\n",
        "for segments, embedding, duration in test_data:\n",
        "    test_xs.append(len(segments))\n",
        "    test_ys.append(duration)\n",
        "test_xs = torch.Tensor(test_xs).reshape(-1, 1).to(device)\n",
        "test_ys = torch.Tensor(test_ys).reshape(-1, 1).to(device)\n",
        "test_loss = test_criterion(model(test_xs), test_ys).detach()\n",
        "print(f\"Final loss on the test data is: {loss}\") # tensor(0.6382)\n",
        "\n",
        "#####################################################################\n",
        "### MODEL WITH LENGTH IN NUMBER OF SEGMENTS PLUS WORD EMBEDDINGS ****\n",
        "EMBEDDING_SIZE = 50\n",
        "model = LinearModel(INPUT_SIZE + EMBEDDING_SIZE, OUTPUT_SIZE)\n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "    criterion = torch.nn.MSELoss() \n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "    # make sure we \"zero out\" the loss at each time step\n",
        "    optimizer.zero_grad()\n",
        "    batch_data = next(iter(train_dataloader))\n",
        "    xs, ys = [], []\n",
        "    for segments, embedding, duration in batch_data:\n",
        "        n_segments = len(segments)\n",
        "        # combine two input spaces\n",
        "        big_x = torch.concat([torch.Tensor([n_segments]), torch.Tensor(embedding)])\n",
        "        xs.append(big_x)\n",
        "        ys.append(duration)\n",
        "    xs = torch.stack(xs).to(device)\n",
        "    ys = torch.Tensor(ys).reshape(-1, 1).to(device)\n",
        "    loss = criterion(model(xs), ys)\n",
        "    if i % EVERY==0:\n",
        "        print(f\"Loss: {loss}\")\n",
        "    # do backprop over that loss\n",
        "    loss.backward()\n",
        "    b, m = model.parameters()\n",
        "    #if i % EVERY==0:\n",
        "        #print(f\"Intercept: {b.detach()[0]}, Slope: {m.detach()[0]}\")\n",
        "    # move on to the next time step\n",
        "    optimizer.step()\n",
        "\n",
        "## TEST ##\n",
        "test_criterion = torch.nn.MSELoss()\n",
        "test_xs, test_ys = [], []\n",
        "for segments, embedding, duration in batch_data:\n",
        "    n_segments = len(segments)\n",
        "    # combine two input spaces\n",
        "    big_x = torch.concat([torch.Tensor([n_segments]), torch.Tensor(embedding)])\n",
        "    test_xs.append(big_x)\n",
        "    test_ys.append(duration)\n",
        "test_xs = torch.stack(test_xs).to(device)\n",
        "test_ys = torch.Tensor(test_ys).reshape(-1, 1).to(device)\n",
        "test_loss = test_criterion(model(test_xs), test_ys).detach()\n",
        "print(f\"Final loss on the test data is: {loss}\") # 0.6157650947570801"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3U5MH2rc5tF",
        "outputId": "3d8bb8f7-0b71-4a9e-f07f-86f2b5a0f2da"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 72.12396240234375\n",
            "Loss: 2.266043186187744\n",
            "Loss: 0.5273388624191284\n",
            "Loss: 0.684627115726471\n",
            "Loss: 0.8294935822486877\n",
            "Loss: 0.7337585687637329\n",
            "Loss: 0.43014270067214966\n",
            "Loss: 0.4064076840877533\n",
            "Loss: 0.6353008151054382\n",
            "Loss: 0.5439980030059814\n",
            "Final loss on the test data is: 0.7615457773208618\n",
            "Loss: 57.227073669433594\n",
            "Loss: 4.095346450805664\n",
            "Loss: 1.4785360097885132\n",
            "Loss: 0.6148879528045654\n",
            "Loss: 0.3057832717895508\n",
            "Loss: 0.7791650891304016\n",
            "Loss: 0.49807560443878174\n",
            "Loss: 0.6263979077339172\n",
            "Loss: 0.23934507369995117\n",
            "Loss: 0.3244963586330414\n",
            "Final loss on the test data is: 0.28784459829330444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW Model"
      ],
      "metadata": {
        "id": "hiGFpcgbc6KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from collections import Counter\n",
        "from train_test_models import BuckeyeDataset, DataLoader\n",
        "import pickle\n",
        "import dill\n",
        "from word2vec import Vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "OUTPUT_SIZE = 1         # total word duration\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0\n",
        "MOMENTUM = 0.001\n",
        "MAX_LOSS = 1.\n",
        "MAX_NORM = 1.\n",
        "N_EPOCHS = 4\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "DEVICE = 'cpu'\n",
        "EMBEDDING_SIZE = 8\n",
        "MAX_NORM = 1\n",
        "EVERY = 1000\n",
        "#EMBEDDING_FILE = './data/phone_weights.pt'\n",
        "EMBEDDING_FILE = './lab_2_data/word2_Vec_Lab_model.pt'\n",
        "#PHONE_VOCAB_FILE = './data/phones.vocab'\n",
        "PHONE_VOCAB_FILE = './lab_2_data/word2_Vec_Lab_model.vocab'\n",
        "TRAIN_PATH = './data/train.jsonl'\n",
        "TEST_PATH = './data/test.jsonl'\n",
        "VECS_PATH = \"./data/buckeye.vecs\"\n",
        "\n",
        "# Fetching the data\n",
        "training_data = BuckeyeDataset(TRAIN_PATH, VECS_PATH)\n",
        "test_data = BuckeyeDataset(TEST_PATH, VECS_PATH)\n",
        "train_dataloader = DataLoader(\n",
        "    training_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE, collate_fn=lambda x: x\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_data, batch_size=BATCH_SIZE, collate_fn= lambda x:x\n",
        ")\n",
        "#phone_embeds = torch.load(EMBEDDING_FILE)\n",
        "\n",
        "\n",
        "#phone_embeds, phone_vocab = train_model()\n",
        "#embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "\n",
        "class LinearModel(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "class Vocab():\n",
        "    def __init__(self, segments: list):\n",
        "        self._compute_frequency_table(segments)\n",
        "        print(self.frequency_table)\n",
        "        self._build_ix_to_vocab_dicts()\n",
        "\n",
        "    def _compute_frequency_table(self, segments):\n",
        "        self.frequency_table = Counter(segments)\n",
        "        self.vocab_size = len(self.frequency_table)\n",
        "    \n",
        "    def _build_ix_to_vocab_dicts(self):\n",
        "        self.ix_to_vocab = {\n",
        "            i: phone for i, phone in enumerate(self.frequency_table)\n",
        "            if self.frequency_table[phone] > 0\n",
        "        }\n",
        "        self.vocab_to_ix = {\n",
        "            self.ix_to_vocab[w]: w for w in self.ix_to_vocab.keys()\n",
        "        }\n",
        "\n",
        "    def tokenize(self, list_of_segments):\n",
        "        return torch.tensor(\n",
        "            [self.vocab_to_ix[w] for w in list_of_segments], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "    \n",
        "    def detokenize(self, tensor):\n",
        "        return torch.tensor(\n",
        "            [self.ix_to_vocab[ix] for ix in tensor], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "class Word2Vec(torch.nn.Module):\n",
        "    def __init__(self, input_size: int, embedding_size: int, output_size: int=None, max_norm=None):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(\n",
        "            input_size,\n",
        "            embedding_size,\n",
        "            max_norm=MAX_NORM\n",
        "            )\n",
        "        if output_size is None:\n",
        "            self.linear = torch.nn.Linear(embedding_size, input_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(embedding_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.mean(axis=1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultilayerDurationModel(torch.nn.Module):\n",
        "    def __init__(self, input_size: int, output_size: int=None, max_norm=None):\n",
        "        super(MultilayerDurationModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, 64) # flat\n",
        "        self.hidden = torch.nn.Linear(64, output_size)\n",
        "        # TODO: Add an intermediate layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.linear(x))\n",
        "        x = self.hidden(x)\n",
        "        # TODO: Add the processing of the intermediate layer\n",
        "        return x\n",
        "\n",
        "\n",
        "def tokenize(vocab, list_of_segments):\n",
        "    return torch.tensor(\n",
        "        [vocab.vocab_to_ix[w] for w in list_of_segments if w in vocab.frequency_table], dtype=torch.long,\n",
        "        device=DEVICE).view(-1, 1)\n",
        "\n",
        "# Fetching the vocab and word2Vec model\n",
        "phone_vocab = dill.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "phone_embeds = Word2Vec(\n",
        "        input_size=phone_vocab.vocab_size,\n",
        "        embedding_size=EMBEDDING_SIZE,\n",
        "        max_norm=MAX_NORM\n",
        "        )\n",
        "phone_embeds.load_state_dict(torch.load(EMBEDDING_FILE))\n",
        "embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "\n",
        "####################################################\n",
        "\n",
        "print(\"Training The Model \")\n",
        "max_training_length = max([len(s[0]) for s in training_data]) # TODO: Figure out this number from the training data\n",
        "\n",
        "#####################################################################\n",
        "### LINEAR MODEL EMBEDDINGS CONCATENATED FOR EACH SEGMENT        ****\n",
        "#####################################################################\n",
        "\n",
        "modelL = LinearModel(max_training_length * embedding_dim, OUTPUT_SIZE).to(DEVICE)\n",
        "\n",
        "#model = LinearModel(embedding_dim, OUTPUT_SIZE)  # TODO: Figure out the right shape\n",
        "embedding_matrix = phone_embeds.embedding.weight.detach()\n",
        "input_dims = max_training_length * embedding_dim\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(modelL.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "    print(\"EPOCH : \" + str(i))\n",
        "    running_loss = 0.0\n",
        "    for idx, batch_data in enumerate(train_dataloader, 0):\n",
        "        xs, ys = [], []\n",
        "\n",
        "        # make sure we \"zero out\" the loss at each time step\n",
        "        optimizer.zero_grad()\n",
        "        batch_segments = []\n",
        "        for segments, embedding, duration in batch_data:\n",
        "            # TODO: concatenate the phone embeddings associated with all segments\n",
        "            # TIP: reference your pt & vocab files\n",
        "            segments_embed = []\n",
        "            if len(segments) > 0:\n",
        "                #concatenating all the segment embedding into a single tensor\n",
        "                for segment in segments:\n",
        "                    segment_id = tokenize(phone_vocab, segment)\n",
        "                    segment_embedding = embedding_matrix[segment_id].flatten().tolist()\n",
        "                    segments_embed.append(segment_embedding)\n",
        "            #embeds = torch.Tensor(np.array(np.concatenate(segments_embed).flat)).flatten()\n",
        "            embeds = list(np.concatenate(segments_embed).flat)\n",
        "            batch_segments.append(embeds)\n",
        "\n",
        "            #Adding padding to make it of the size of largest sequence tensor\n",
        "            #padding_len = int((input_dims - (embeds.shape[0]))/2)\n",
        "            #padding = torch.nn.ConstantPad1d(padding_len, 0.)\n",
        "\n",
        "            #ConstPad1d() takes care of tensors that are bigger than max size.\n",
        "            #padded_embeds = padding(embeds)\n",
        "            ys.append(duration)\n",
        "\n",
        "        # Adding padding to make it of the size of largest sequence tensor\n",
        "        padded_embeds = torch.Tensor(pad_sequences(batch_segments, value=0., padding='post', truncating='post', maxlen=input_dims, dtype='float32'))\n",
        "        #xs = torch.stack(xs)\n",
        "        ys = torch.Tensor(ys).reshape(-1, 1).to(DEVICE)\n",
        "        loss = criterion(modelL(padded_embeds), ys)\n",
        "        running_loss += loss.item()\n",
        "        if idx % EVERY==0:\n",
        "            print(f\"Loss: {loss}\")\n",
        "\n",
        "        # do backprop over that loss\n",
        "        loss.backward()\n",
        "        #b, m = model.parameters()\n",
        "        #if i % EVERY==0:\n",
        "        #    print(f\"Intercept: {b.detach()[0]}, Slope: {m.detach()[0]}\")\n",
        "        # move on to the next time step\n",
        "        optimizer.step()\n",
        "    print(loss.item())\n",
        "\n",
        "print(\"\\n DONE Training \")\n",
        "\n",
        "## TEST ##\n",
        "print(\"\\n \\n Evaluating the Linear model on Test Data\")\n",
        "modelL.eval()\n",
        "\n",
        "for idx, batch_data in enumerate(test_dataloader, 0):\n",
        "    xs, ys = [], []\n",
        "    optimizer.zero_grad()\n",
        "    batch_segments = []\n",
        "    for segments, embedding, duration in batch_data:\n",
        "        # TODO: concatenate the phone embeddings associated with all segments\n",
        "        # TIP: reference your pt & vocab files\n",
        "        segments_embed = []\n",
        "        if len(segments) > 0:\n",
        "            # concatenating all the segment embedding into a single tensor\n",
        "            for segment in segments:\n",
        "                segment_id = tokenize(phone_vocab, segment)\n",
        "                segment_embedding = embedding_matrix[segment_id].flatten().tolist()\n",
        "                segments_embed.append(segment_embedding)\n",
        "\n",
        "        embeds = list(np.concatenate(segments_embed).flat)\n",
        "        batch_segments.append(embeds)\n",
        "\n",
        "        # Adding padding to make it of the size of largest sequence tensor\n",
        "        #padding_len = int((input_dims - (embeds.shape[0])) / 2)\n",
        "        #padding = torch.nn.ConstantPad1d(padding_len, 0.)\n",
        "        ys.append(duration)\n",
        "\n",
        "    padded_embeds = torch.Tensor(pad_sequences(batch_segments, value=0., padding='post', truncating='post', maxlen=input_dims, dtype='float32'))\n",
        "    ys = torch.Tensor(ys).reshape(-1, 1).to(DEVICE)\n",
        "    loss = criterion(modelL(padded_embeds), ys)\n",
        "    if idx % EVERY == 0:\n",
        "        print(f\"Loss: {loss}\")\n",
        "\n",
        "print(f\"Final loss on the test data  for linear model is: {loss}\")\n",
        "torch.save(modelL.state_dict(), \"./data/cbow_linear_model.pt\")\n",
        "\n",
        "#####################################################################\n",
        "### MULTILAYER MODEL EMBEDDINGS CONCATENATED FOR EACH SEGMENT    ****\n",
        "#####################################################################\n",
        "\n",
        "modelM = MultilayerDurationModel(max_training_length * embedding_dim, OUTPUT_SIZE).to(DEVICE)\n",
        "#model = LinearModel(embedding_dim, OUTPUT_SIZE)  # TODO: Figure out the right shape\n",
        "embedding_matrix = phone_embeds.embedding.weight.detach()\n",
        "input_dims = max_training_length * embedding_dim\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(modelM.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "    print(\"EPOCH : \" + str(i))\n",
        "    running_loss = 0.0\n",
        "    for idx, batch_data in enumerate(train_dataloader, 0):\n",
        "        xs, ys = [], []\n",
        "\n",
        "        # make sure we \"zero out\" the loss at each time step\n",
        "        optimizer.zero_grad()\n",
        "        batch_segments = []\n",
        "        for segments, embedding, duration in batch_data:\n",
        "            # TODO: concatenate the phone embeddings associated with all segments\n",
        "            # TIP: reference your pt & vocab files\n",
        "            segments_embed = []\n",
        "            if len(segments) > 0:\n",
        "                #concatenating all the segment embedding into a single tensor\n",
        "                for segment in segments:\n",
        "                    segment_id = tokenize(phone_vocab, segment)\n",
        "                    segment_embedding = embedding_matrix[segment_id].flatten().tolist()\n",
        "                    segments_embed.append(segment_embedding)\n",
        "            #embeds = torch.Tensor(np.array(np.concatenate(segments_embed).flat)).flatten()\n",
        "            embeds = list(np.concatenate(segments_embed).flat)\n",
        "            batch_segments.append(embeds)\n",
        "\n",
        "            #Adding padding to make it of the size of largest sequence tensor\n",
        "            #padding_len = int((input_dims - (embeds.shape[0]))/2)\n",
        "            #padding = torch.nn.ConstantPad1d(padding_len, 0.)\n",
        "\n",
        "            #ConstPad1d() takes care of tensors that are bigger than max size.\n",
        "            #padded_embeds = padding(embeds)\n",
        "            ys.append(duration)\n",
        "\n",
        "        # Adding padding to make it of the size of largest sequence tensor\n",
        "        padded_embeds = torch.Tensor(pad_sequences(batch_segments, value=0., padding='post', truncating='post', maxlen=input_dims, dtype='float32'))\n",
        "        #xs = torch.stack(xs)\n",
        "        ys = torch.Tensor(ys).reshape(-1, 1).to(DEVICE)\n",
        "        loss = criterion(modelM(padded_embeds), ys)\n",
        "        running_loss += loss.item()\n",
        "        if idx % EVERY==0:\n",
        "            print(f\"Loss: {loss}\")\n",
        "\n",
        "        # do backprop over that loss\n",
        "        loss.backward()\n",
        "        #b, m = model.parameters()\n",
        "        #if i % EVERY==0:\n",
        "        #    print(f\"Intercept: {b.detach()[0]}, Slope: {m.detach()[0]}\")\n",
        "        # move on to the next time step\n",
        "        optimizer.step()\n",
        "    print(loss.item())\n",
        "\n",
        "print(\"\\n DONE Training \")\n",
        "\n",
        "## TEST ##\n",
        "print(\"\\n \\n Evaluating the Multi layer model on Test Data\")\n",
        "modelM.eval()\n",
        "\n",
        "for idx, batch_data in enumerate(test_dataloader, 0):\n",
        "    xs, ys = [], []\n",
        "    optimizer.zero_grad()\n",
        "    batch_segments = []\n",
        "    for segments, embedding, duration in batch_data:\n",
        "        # TODO: concatenate the phone embeddings associated with all segments\n",
        "        # TIP: reference your pt & vocab files\n",
        "        segments_embed = []\n",
        "        if len(segments) > 0:\n",
        "            # concatenating all the segment embedding into a single tensor\n",
        "            for segment in segments:\n",
        "                segment_id = tokenize(phone_vocab, segment)\n",
        "                segment_embedding = embedding_matrix[segment_id].flatten().tolist()\n",
        "                segments_embed.append(segment_embedding)\n",
        "\n",
        "        embeds = list(np.concatenate(segments_embed).flat)\n",
        "        batch_segments.append(embeds)\n",
        "\n",
        "        # Adding padding to make it of the size of largest sequence tensor\n",
        "        #padding_len = int((input_dims - (embeds.shape[0])) / 2)\n",
        "        #padding = torch.nn.ConstantPad1d(padding_len, 0.)\n",
        "        ys.append(duration)\n",
        "\n",
        "    padded_embeds = torch.Tensor(pad_sequences(batch_segments, value=0., padding='post', truncating='post', maxlen=input_dims, dtype='float32'))\n",
        "    ys = torch.Tensor(ys).reshape(-1, 1).to(DEVICE)\n",
        "    loss = criterion(modelM(padded_embeds), ys)\n",
        "    if idx % EVERY == 0:\n",
        "        print(f\"Loss: {loss}\")\n",
        "print(f\"Final loss on the test data for multilayer model is: {loss}\")\n",
        "\n",
        "torch.save(modelM.state_dict(), \"./data/cbow_multi_model.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Y1aov5c9Hh",
        "outputId": "17f932ff-df6e-46bc-bf5e-3af174ff1392"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training The Model \n",
            "EPOCH : 0\n",
            "Loss: 62.66505813598633\n",
            "Loss: 1.7216973304748535\n",
            "Loss: 1.0308253765106201\n",
            "Loss: 1.547096848487854\n",
            "Loss: 1.2050435543060303\n",
            "0.48198163509368896\n",
            "EPOCH : 1\n",
            "Loss: 0.5772705078125\n",
            "Loss: 0.5946406126022339\n",
            "Loss: 0.6033220887184143\n",
            "Loss: 0.5743860006332397\n",
            "Loss: 0.6662980318069458\n",
            "0.5000657439231873\n",
            "EPOCH : 2\n",
            "Loss: 0.47570523619651794\n",
            "Loss: 0.49491262435913086\n",
            "Loss: 0.7599642276763916\n",
            "Loss: 0.6301745772361755\n",
            "Loss: 0.6011781692504883\n",
            "0.7191373109817505\n",
            "EPOCH : 3\n",
            "Loss: 0.4255598187446594\n",
            "Loss: 0.643860936164856\n",
            "Loss: 0.405662477016449\n",
            "Loss: 0.3001071512699127\n",
            "Loss: 0.43268153071403503\n",
            "0.5643084049224854\n",
            "\n",
            " DONE Training \n",
            "\n",
            " \n",
            " Evaluating the Linear model on Test Data\n",
            "Loss: 0.6381330490112305\n",
            "Loss: 0.5770381689071655\n",
            "Final loss on the test data  for linear model is: 0.18668614327907562\n",
            "EPOCH : 0\n",
            "Loss: 57.52842330932617\n",
            "Loss: 0.5226792097091675\n",
            "Loss: 0.4405328929424286\n",
            "Loss: 0.33546605706214905\n",
            "Loss: 0.6175293922424316\n",
            "0.6169184446334839\n",
            "EPOCH : 1\n",
            "Loss: 0.33108535408973694\n",
            "Loss: 0.8668553233146667\n",
            "Loss: 0.6128934621810913\n",
            "Loss: 0.4870290160179138\n",
            "Loss: 0.6058155298233032\n",
            "0.5393666625022888\n",
            "EPOCH : 2\n",
            "Loss: 0.611931324005127\n",
            "Loss: 0.5553630590438843\n",
            "Loss: 0.7607165575027466\n",
            "Loss: 0.4855133593082428\n",
            "Loss: 0.2496010810136795\n",
            "0.7669317722320557\n",
            "EPOCH : 3\n",
            "Loss: 0.46085959672927856\n",
            "Loss: 0.2963237464427948\n",
            "Loss: 0.2606274485588074\n",
            "Loss: 0.4647333323955536\n",
            "Loss: 0.6593064069747925\n",
            "0.5365388989448547\n",
            "\n",
            " DONE Training \n",
            "\n",
            " \n",
            " Evaluating the Multi layer model on Test Data\n",
            "Loss: 0.543217122554779\n",
            "Loss: 0.43637615442276\n",
            "Final loss on the test data for multilayer model is: 0.10425885021686554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-Decoder "
      ],
      "metadata": {
        "id": "2SxGIdilVZuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import pickle, time, math, random, json\n",
        "#from train_test_models import BuckeyeDataset, DataLoader\n",
        "import dill\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "OUTPUT_SIZE = 1  # total word duration\n",
        "MAX_LENGTH = 18\n",
        "HIDDEN_SIZE = 256\n",
        "DEVICE = 'cpu'\n",
        "MAX_NORM = 1\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0\n",
        "MOMENTUM = 0.0001\n",
        "MAX_LOSS = 1.\n",
        "N_EPOCHS = 300\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "EMBEDDING_SIZE = 8\n",
        "EVERY = 30\n",
        "GLOVE_EMBED_DIM=50\n",
        "\n",
        "EMBEDDING_FILE = './lab_2_data/word2_Vec_Lab_model.pt'\n",
        "PHONE_VOCAB_FILE = './lab_2_data/word2_Vec_Lab_model.vocab'\n",
        "TRAIN_PATH = './data/train.jsonl'\n",
        "TEST_PATH = './data/test.jsonl'\n",
        "VECS_PATH = \"./data/buckeye.vecs\"\n",
        "SHUFFLE_DATA = True\n",
        "\n",
        "def tokenize(vocab, list_of_segments):\n",
        "    return torch.tensor(\n",
        "        [vocab.vocab_to_ix[w] for w in list_of_segments if w in vocab.frequency_table] + [EOS_TOKEN], dtype=torch.long,\n",
        "        device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "def process_segments_for_encoder(line_data):\n",
        "    # Split every line into pairs and normalize\n",
        "    split_str = line_data['observed_pron'].split(\" \")\n",
        "    duration = torch.log(torch.Tensor([sum(line_data['segment_duration_ms'])]))\n",
        "    return (split_str, duration)\n",
        "\n",
        "\n",
        "def tensorFromSentence(vocab, sentence):\n",
        "    indices = tokenize(vocab, sentence)\n",
        "    return indices\n",
        "\n",
        "\n",
        "class Vocab():\n",
        "    def __init__(self, segments: list):\n",
        "        self._compute_frequency_table(segments)\n",
        "        print(self.frequency_table)\n",
        "        self._build_ix_to_vocab_dicts()\n",
        "\n",
        "    def _compute_frequency_table(self, segments):\n",
        "        self.frequency_table = Counter(segments)\n",
        "        self.vocab_size = len(self.frequency_table)\n",
        "\n",
        "    def _build_ix_to_vocab_dicts(self):\n",
        "        self.ix_to_vocab = {\n",
        "            i: phone for i, phone in enumerate(self.frequency_table)\n",
        "            if self.frequency_table[phone] > 0\n",
        "        }\n",
        "        self.vocab_to_ix = {\n",
        "            self.ix_to_vocab[w]: w for w in self.ix_to_vocab.keys()\n",
        "        }\n",
        "\n",
        "    def tokenize(self, list_of_segments):\n",
        "        return torch.tensor(\n",
        "            [self.vocab_to_ix[w] for w in list_of_segments], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "    def detokenize(self, tensor):\n",
        "        return torch.tensor(\n",
        "            [self.ix_to_vocab[ix] for ix in tensor], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "class Word2Vec(torch.nn.Module):\n",
        "    def __init__(self, input_size: int, embedding_size: int, output_size: int = None, max_norm=None):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(\n",
        "            input_size,\n",
        "            embedding_size,\n",
        "            max_norm=MAX_NORM\n",
        "        )\n",
        "        if output_size is None:\n",
        "            self.linear = torch.nn.Linear(embedding_size, input_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(embedding_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.mean(axis=1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class EncoderGloveRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, glove_size=None):\n",
        "        super(EncoderGloveRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        if glove_size is not None:\n",
        "            self.linear = torch.nn.Linear(glove_size, hidden_size)\n",
        "            self.gru = torch.nn.GRU(hidden_size * 2, hidden_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(0, hidden_size)\n",
        "            self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden, glove_embedding=None):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        if glove_embedding is not None:\n",
        "            linear = self.linear(glove_embedding).view(1, 1, -1)\n",
        "            #print(embedded.shape, linear.shape)\n",
        "            output = torch.cat((embedded, linear), axis=2)\n",
        "        else:\n",
        "            output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class EncoderRNN3(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embeddings=None):\n",
        "        super(EncoderRNN3, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        if embeddings is not None:\n",
        "            input_size, hidden_size = embeddings.size()\n",
        "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "            self.embedding.weight = torch.nn.Parameter(embeddings)\n",
        "        else:\n",
        "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class DecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = torch.nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = torch.nn.Dropout(self.dropout_p)\n",
        "        self.gru = torch.nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
        "          max_length=MAX_LENGTH, glove_embedding=None):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max(input_length, max_length), encoder.hidden_size, device=DEVICE)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        #encoder_output, encoder_hidden = encoder(\n",
        "        #    input_tensor[ei], encoder_hidden, glove_embedding)\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_TOKEN]], device=DEVICE)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # use its own predictions as the next input\n",
        "    for di in range(target_length):\n",
        "        if type(decoder) is DecoderRNN:\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "        elif type(decoder) is AttnDecoderRNN:\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "        loss += criterion(decoder_output, target_tensor[di].view(1))\n",
        "        if decoder_input.item() == EOS_TOKEN:\n",
        "            break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return encoder, decoder, loss.item() / target_length\n",
        "\n",
        "\n",
        "def trainIters(pairs, encoder, decoder, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "\n",
        "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "\n",
        "    for training_pair in pairs:\n",
        "        #input_tensor, target_tensor, glove_embedding = training_pair\n",
        "        input_tensor, target_tensor = training_pair\n",
        "\n",
        "        #encoder, decoder, loss = train(\n",
        "        #    input_tensor, target_tensor, encoder,\n",
        "        #    decoder, encoder_optimizer, decoder_optimizer, criterion, glove_embedding=torch.Tensor(glove_embedding))\n",
        "        encoder, decoder, loss = train(\n",
        "            input_tensor, target_tensor, encoder,\n",
        "            decoder, encoder_optimizer, decoder_optimizer, criterion,)\n",
        "\n",
        "\n",
        "class LinearModel(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "def main():\n",
        "\n",
        "    #fetching the data\n",
        "    training_data = BuckeyeDataset(TRAIN_PATH, VECS_PATH)\n",
        "    test_data = BuckeyeDataset(TEST_PATH, VECS_PATH)\n",
        "    train_dataloader = DataLoader(\n",
        "        training_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE_DATA, collate_fn=lambda x: x\n",
        "    )\n",
        "    # loading the Vocab class\n",
        "    phone_vocab = dill.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "\n",
        "    # loading learned phone embeddings from CBOW\n",
        "    phone_embeds = Word2Vec(\n",
        "        input_size=phone_vocab.vocab_size,\n",
        "        embedding_size=EMBEDDING_SIZE,\n",
        "        max_norm=MAX_NORM\n",
        "    )\n",
        "    phone_embeds.load_state_dict(torch.load(EMBEDDING_FILE))\n",
        "    embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "\n",
        "    #phone_embeds = torch.load(EMBEDDING_FILE)\n",
        "\n",
        "\n",
        "    #phone_vocab = pickle.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "    #embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "    #evaluating the max length\n",
        "    MAX_LENGTH = max([len(s[0]) for s in training_data])\n",
        "\n",
        "\n",
        "    # Encoder-decoder hidden states\n",
        "    # Encoder-decoder hidden states + learned CBOW embeddings\n",
        "    # Encoder-decoder hidden states + learned phone embeddings + GloVe embeddings\n",
        "    # Encoder-decoder with attention hidden states\n",
        "    # Encoder-decoder with attention hidden states + GloVe embeddings\n",
        "    # Encoder-decoder with attention hidden states + learned CBOW embeddings + GloVe embeddings\n",
        "\n",
        "    n_words_for_encoders = phone_vocab.vocab_size + len([SOS_TOKEN, EOS_TOKEN])\n",
        "    encoder1 = EncoderRNN(n_words_for_encoders, HIDDEN_SIZE).to(DEVICE)\n",
        "    encoder2 = EncoderGloveRNN(n_words_for_encoders, HIDDEN_SIZE, GLOVE_EMBED_DIM).to(DEVICE)\n",
        "    encoder3 = EncoderRNN3(n_words_for_encoders, HIDDEN_SIZE,\n",
        "                           embeddings=torch.nn.Parameter(phone_embeds.embedding.weight)).to(DEVICE)\n",
        "\n",
        "    decoder1 = DecoderRNN(HIDDEN_SIZE, n_words_for_encoders)\n",
        "    attn_decoder1 = AttnDecoderRNN(HIDDEN_SIZE, n_words_for_encoders, dropout_p=0.1).to(DEVICE)\n",
        "    print(\"Training Encoder Decoder\")\n",
        "    for encoder in [encoder1]:#  , encoder3]: # , encoder2]:\n",
        "        for decoder in [decoder1]: #, attn_decoder1]:\n",
        "            for i in range(N_EPOCHS):\n",
        "                batch_data = next(iter(train_dataloader))\n",
        "                xs, ys, glove_embeddings = [], [], []\n",
        "                for segments, glove_embedding, duration in batch_data:\n",
        "                    if len(segments) > 0:\n",
        "                        xs.append(tensorFromSentence(phone_vocab, segments))\n",
        "                        ys.append(tensorFromSentence(phone_vocab, segments))\n",
        "                        glove_embeddings.append(glove_embedding)\n",
        "                #print(glove_embedding, embedding_dim)\n",
        "                #pairs = list(zip(xs, ys, glove_embeddings))\n",
        "                pairs = list(zip(xs, ys))\n",
        "                trainIters(pairs, encoder, decoder)\n",
        "                # evaluateRandomly(encoder, decoder, pairs, phone_vocab, n=10)\n",
        "                encoder_type = str(type(encoder)).split(\".\")[-1][:-2]\n",
        "                decoder_type = str(type(decoder)).split(\".\")[-1][:-2]\n",
        "                #torch.save(encoder, f\"./2022-05-11_{type(encoder)}_{type(decoder)}.pt\")\n",
        "                torch.save(encoder, f\"./2022-05-11_{encoder_type}_{decoder_type}.pt\")\n",
        "                #torch.save(encoder.state_dict(), f\"./2022-05-11_{type(encoder)}_{type(decoder)}.pt\")\n",
        "\n",
        "    # train duration prediction model\n",
        "    model = LinearModel(HIDDEN_SIZE, OUTPUT_SIZE)\n",
        "    embedding_matrix = phone_embeds.embedding.weight.detach()\n",
        "    print(\"Training model\")\n",
        "    for ix, encoder in enumerate([encoder1]):# , encoder3]): #, encoder3]):\n",
        "        encoder.eval()\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        for i in range(N_EPOCHS):\n",
        "            loss = 0\n",
        "            criterion = torch.nn.MSELoss()\n",
        "            #optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "            # make sure we \"zero out\" the loss at each time step\n",
        "            optimizer.zero_grad()\n",
        "            batch_data = next(iter(train_dataloader))\n",
        "            xs, ys = [], []\n",
        "            batch_segments = []\n",
        "            #for idx, batch_data in enumerate(train_dataloader, 0):\n",
        "            for segments, embedding, duration in batch_data:\n",
        "              input_length = len(segments)\n",
        "              encoder_outputs = torch.zeros(input_length, HIDDEN_SIZE,\n",
        "                                              device=DEVICE, requires_grad=False)\n",
        "              if input_length > 0:\n",
        "                input_tensor = tokenize(phone_vocab, segments)\n",
        "                for ei in range(input_length):\n",
        "                  #encoder_output, encoder_hidden = encoder(\n",
        "                  #              input_tensor[ei], encoder_hidden, torch.Tensor(embedding))\n",
        "                  encoder_output, encoder_hidden = encoder(\n",
        "                                input_tensor[ei], encoder_hidden)\n",
        "                  encoder_outputs[ei] += encoder_output[0, 0]\n",
        "                  last_hidden_state = encoder_outputs[-1].detach()\n",
        "                  xs.append(last_hidden_state.flatten())\n",
        "                  ys.append(duration)\n",
        "            xs = torch.stack(xs)\n",
        "            ys = torch.Tensor(ys).reshape(-1, 1)\n",
        "            loss = criterion(model(xs), ys)\n",
        "\n",
        "            if i % EVERY == 0:\n",
        "                print(f\"Loss: {loss}\")\n",
        "\n",
        "            loss.backward()\n",
        "            b, m = model.parameters()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        print(f\"Final loss on the training data for encoder {ix} is: {loss}\")  # tensor(0.478)\n",
        "        ## TEST ##\n",
        "        test_criterion = torch.nn.MSELoss()\n",
        "        test_xs, test_ys = [], []\n",
        "        for segments, embedding, duration in batch_data:\n",
        "            input_length = len(segments)\n",
        "            encoder_outputs = torch.zeros(input_length, HIDDEN_SIZE,\n",
        "                                          device=DEVICE, requires_grad=False)\n",
        "            if input_length > 0:\n",
        "                input_tensor = tokenize(phone_vocab, segments)\n",
        "                for ei in range(input_length):\n",
        "                    #encoder_output, encoder_hidden = encoder(\n",
        "                    #    input_tensor[ei], encoder_hidden, torch.Tensor(embedding))\n",
        "                    encoder_output, encoder_hidden = encoder(\n",
        "                        input_tensor[ei], encoder_hidden)\n",
        "                    encoder_outputs[ei] += encoder_output[0, 0]\n",
        "                last_hidden_state = encoder_outputs[-1].detach()\n",
        "            test_xs.append(last_hidden_state)\n",
        "            test_ys.append(duration)\n",
        "        test_xs = torch.stack(test_xs)\n",
        "        test_ys = torch.Tensor(test_ys).reshape(-1, 1)\n",
        "        test_loss = test_criterion(model(test_xs), test_ys).detach()\n",
        "        \n",
        "        print(f\"Final loss on the test data for encoder {ix} is: {test_loss}\")  # tensor(0.7235)"
      ],
      "metadata": {
        "id": "LrwSZukXVeXY"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0AClmNCVepM",
        "outputId": "f984faea-d50a-4c32-d237-cf7a70069643"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Encoder Decoder\n",
            "Training model\n",
            "Loss: 78.26979064941406\n",
            "Loss: 0.5052607655525208\n",
            "Loss: 0.4616890847682953\n",
            "Loss: 0.5968324542045593\n",
            "Loss: 0.4828501343727112\n",
            "Loss: 0.5633633732795715\n",
            "Loss: 0.5704646706581116\n",
            "Loss: 0.35789021849632263\n",
            "Loss: 0.2999202013015747\n",
            "Loss: 0.504475474357605\n",
            "Final loss on the training data for encoder 0 is: 0.5097392797470093\n",
            "Final loss on the test data for encoder 0 is: 0.6795893907546997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-Decoder with CBOW Embeddings"
      ],
      "metadata": {
        "id": "bRdpoUAxVfBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import pickle, time, math, random, json\n",
        "#from train_test_models import BuckeyeDataset, DataLoader\n",
        "import dill\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "OUTPUT_SIZE = 1  # total word duration\n",
        "MAX_LENGTH = 18\n",
        "HIDDEN_SIZE = 8\n",
        "DEVICE = 'cpu'\n",
        "MAX_NORM = 1\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "WEIGHT_DECAY = 0\n",
        "MOMENTUM = 0.0001\n",
        "MAX_LOSS = 1.\n",
        "N_EPOCHS = 300\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "EMBEDDING_SIZE = 8\n",
        "EVERY = 30\n",
        "GLOVE_EMBED_DIM=50\n",
        "\n",
        "EMBEDDING_FILE = './lab_2_data/word2_Vec_Lab_model.pt'\n",
        "PHONE_VOCAB_FILE = './lab_2_data/word2_Vec_Lab_model.vocab'\n",
        "TRAIN_PATH = './data/train.jsonl'\n",
        "TEST_PATH = './data/test.jsonl'\n",
        "VECS_PATH = \"./data/buckeye.vecs\"\n",
        "SHUFFLE_DATA = True\n",
        "\n",
        "def tokenize(vocab, list_of_segments):\n",
        "    return torch.tensor(\n",
        "        [vocab.vocab_to_ix[w] for w in list_of_segments if w in vocab.frequency_table] + [EOS_TOKEN], dtype=torch.long,\n",
        "        device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "def process_segments_for_encoder(line_data):\n",
        "    # Split every line into pairs and normalize\n",
        "    split_str = line_data['observed_pron'].split(\" \")\n",
        "    duration = torch.log(torch.Tensor([sum(line_data['segment_duration_ms'])]))\n",
        "    return (split_str, duration)\n",
        "\n",
        "\n",
        "def tensorFromSentence(vocab, sentence):\n",
        "    indices = tokenize(vocab, sentence)\n",
        "    return indices\n",
        "\n",
        "\n",
        "class Vocab():\n",
        "    def __init__(self, segments: list):\n",
        "        self._compute_frequency_table(segments)\n",
        "        print(self.frequency_table)\n",
        "        self._build_ix_to_vocab_dicts()\n",
        "\n",
        "    def _compute_frequency_table(self, segments):\n",
        "        self.frequency_table = Counter(segments)\n",
        "        self.vocab_size = len(self.frequency_table)\n",
        "\n",
        "    def _build_ix_to_vocab_dicts(self):\n",
        "        self.ix_to_vocab = {\n",
        "            i: phone for i, phone in enumerate(self.frequency_table)\n",
        "            if self.frequency_table[phone] > 0\n",
        "        }\n",
        "        self.vocab_to_ix = {\n",
        "            self.ix_to_vocab[w]: w for w in self.ix_to_vocab.keys()\n",
        "        }\n",
        "\n",
        "    def tokenize(self, list_of_segments):\n",
        "        return torch.tensor(\n",
        "            [self.vocab_to_ix[w] for w in list_of_segments], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "    def detokenize(self, tensor):\n",
        "        return torch.tensor(\n",
        "            [self.ix_to_vocab[ix] for ix in tensor], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "class Word2Vec(torch.nn.Module):\n",
        "    def __init__(self, input_size: int, embedding_size: int, output_size: int = None, max_norm=None):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(\n",
        "            input_size,\n",
        "            embedding_size,\n",
        "            max_norm=MAX_NORM\n",
        "        )\n",
        "        if output_size is None:\n",
        "            self.linear = torch.nn.Linear(embedding_size, input_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(embedding_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.mean(axis=1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class EncoderGloveRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, glove_size=None):\n",
        "        super(EncoderGloveRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        if glove_size is not None:\n",
        "            self.linear = torch.nn.Linear(glove_size, hidden_size)\n",
        "            self.gru = torch.nn.GRU(hidden_size * 2, hidden_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(0, hidden_size)\n",
        "            self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden, glove_embedding=None):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        if glove_embedding is not None:\n",
        "            linear = self.linear(glove_embedding).view(1, 1, -1)\n",
        "            #print(embedded.shape, linear.shape)\n",
        "            output = torch.cat((embedded, linear), axis=2)\n",
        "        else:\n",
        "            output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class EncoderRNN3(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embeddings=None):\n",
        "        super(EncoderRNN3, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        if embeddings is not None:\n",
        "            input_size, hidden_size = embeddings.size()\n",
        "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "            self.embedding.weight = torch.nn.Parameter(embeddings)\n",
        "        else:\n",
        "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class DecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = torch.nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = torch.nn.Dropout(self.dropout_p)\n",
        "        self.gru = torch.nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
        "          max_length=MAX_LENGTH, glove_embedding=None):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max(input_length, max_length), encoder.hidden_size, device=DEVICE)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        #encoder_output, encoder_hidden = encoder(\n",
        "        #    input_tensor[ei], encoder_hidden, glove_embedding)\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_TOKEN]], device=DEVICE)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # use its own predictions as the next input\n",
        "    for di in range(target_length):\n",
        "        if type(decoder) is DecoderRNN:\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "        elif type(decoder) is AttnDecoderRNN:\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "        loss += criterion(decoder_output, target_tensor[di].view(1))\n",
        "        if decoder_input.item() == EOS_TOKEN:\n",
        "            break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return encoder, decoder, loss.item() / target_length\n",
        "\n",
        "\n",
        "def trainIters(pairs, encoder, decoder, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "\n",
        "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "\n",
        "    for training_pair in pairs:\n",
        "        #input_tensor, target_tensor, glove_embedding = training_pair\n",
        "        input_tensor, target_tensor = training_pair\n",
        "\n",
        "        #encoder, decoder, loss = train(\n",
        "        #    input_tensor, target_tensor, encoder,\n",
        "        #    decoder, encoder_optimizer, decoder_optimizer, criterion, glove_embedding=torch.Tensor(glove_embedding))\n",
        "        encoder, decoder, loss = train(\n",
        "            input_tensor, target_tensor, encoder,\n",
        "            decoder, encoder_optimizer, decoder_optimizer, criterion,)\n",
        "\n",
        "\n",
        "class LinearModel(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "def main():\n",
        "\n",
        "    #fetching the data\n",
        "    training_data = BuckeyeDataset(TRAIN_PATH, VECS_PATH)\n",
        "    test_data = BuckeyeDataset(TEST_PATH, VECS_PATH)\n",
        "    train_dataloader = DataLoader(\n",
        "        training_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE_DATA, collate_fn=lambda x: x\n",
        "    )\n",
        "    # loading the Vocab class\n",
        "    phone_vocab = dill.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "\n",
        "    # loading learned phone embeddings from CBOW\n",
        "    phone_embeds = Word2Vec(\n",
        "        input_size=phone_vocab.vocab_size,\n",
        "        embedding_size=EMBEDDING_SIZE,\n",
        "        max_norm=MAX_NORM\n",
        "    )\n",
        "    phone_embeds.load_state_dict(torch.load(EMBEDDING_FILE))\n",
        "    embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "\n",
        "    #phone_embeds = torch.load(EMBEDDING_FILE)\n",
        "\n",
        "\n",
        "    #phone_vocab = pickle.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "    #embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "    #evaluating the max length\n",
        "    MAX_LENGTH = max([len(s[0]) for s in training_data])\n",
        "\n",
        "\n",
        "    # Encoder-decoder hidden states\n",
        "    # Encoder-decoder hidden states + learned CBOW embeddings\n",
        "    # Encoder-decoder hidden states + learned phone embeddings + GloVe embeddings\n",
        "    # Encoder-decoder with attention hidden states\n",
        "    # Encoder-decoder with attention hidden states + GloVe embeddings\n",
        "    # Encoder-decoder with attention hidden states + learned CBOW embeddings + GloVe embeddings\n",
        "\n",
        "    n_words_for_encoders = phone_vocab.vocab_size + len([SOS_TOKEN, EOS_TOKEN])\n",
        "    encoder1 = EncoderRNN(n_words_for_encoders, HIDDEN_SIZE).to(DEVICE)\n",
        "    encoder2 = EncoderGloveRNN(n_words_for_encoders, HIDDEN_SIZE, GLOVE_EMBED_DIM).to(DEVICE)\n",
        "    encoder3 = EncoderRNN3(n_words_for_encoders, HIDDEN_SIZE,\n",
        "                           embeddings=torch.nn.Parameter(phone_embeds.embedding.weight)).to(DEVICE)\n",
        "\n",
        "    decoder1 = DecoderRNN(HIDDEN_SIZE, n_words_for_encoders)\n",
        "    attn_decoder1 = AttnDecoderRNN(HIDDEN_SIZE, n_words_for_encoders, dropout_p=0.1).to(DEVICE)\n",
        "    print(\"Training Encoder Decoder\")\n",
        "    for encoder in [encoder1, encoder3]:#  , encoder3]: # , encoder2]:\n",
        "        for decoder in [decoder1]: #, attn_decoder1]:\n",
        "            for i in range(N_EPOCHS):\n",
        "                batch_data = next(iter(train_dataloader))\n",
        "                xs, ys, glove_embeddings = [], [], []\n",
        "                for segments, glove_embedding, duration in batch_data:\n",
        "                    if len(segments) > 0:\n",
        "                        xs.append(tensorFromSentence(phone_vocab, segments))\n",
        "                        ys.append(tensorFromSentence(phone_vocab, segments))\n",
        "                        glove_embeddings.append(glove_embedding)\n",
        "                #print(glove_embedding, embedding_dim)\n",
        "                #pairs = list(zip(xs, ys, glove_embeddings))\n",
        "                pairs = list(zip(xs, ys))\n",
        "                trainIters(pairs, encoder, decoder)\n",
        "                # evaluateRandomly(encoder, decoder, pairs, phone_vocab, n=10)\n",
        "                encoder_type = str(type(encoder)).split(\".\")[-1][:-2]\n",
        "                decoder_type = str(type(decoder)).split(\".\")[-1][:-2]\n",
        "                #torch.save(encoder, f\"./2022-05-11_{type(encoder)}_{type(decoder)}.pt\")\n",
        "                torch.save(encoder, f\"./2022-05-11_{encoder_type}_{decoder_type}.pt\")\n",
        "                #torch.save(encoder.state_dict(), f\"./2022-05-11_{type(encoder)}_{type(decoder)}.pt\")\n",
        "\n",
        "    # train duration prediction model\n",
        "    model = LinearModel(HIDDEN_SIZE, OUTPUT_SIZE)\n",
        "    embedding_matrix = phone_embeds.embedding.weight.detach()\n",
        "    print(\"Training model\")\n",
        "    for ix, encoder in enumerate([encoder1, encoder3]):# , encoder3]): #, encoder3]):\n",
        "        encoder.eval()\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        for i in range(N_EPOCHS):\n",
        "            loss = 0\n",
        "            criterion = torch.nn.MSELoss()\n",
        "            #optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "            # make sure we \"zero out\" the loss at each time step\n",
        "            optimizer.zero_grad()\n",
        "            batch_data = next(iter(train_dataloader))\n",
        "            xs, ys = [], []\n",
        "            batch_segments = []\n",
        "            #for idx, batch_data in enumerate(train_dataloader, 0):\n",
        "            for segments, embedding, duration in batch_data:\n",
        "              input_length = len(segments)\n",
        "              encoder_outputs = torch.zeros(input_length, HIDDEN_SIZE,\n",
        "                                              device=DEVICE, requires_grad=False)\n",
        "              if input_length > 0:\n",
        "                input_tensor = tokenize(phone_vocab, segments)\n",
        "                for ei in range(input_length):\n",
        "                  #encoder_output, encoder_hidden = encoder(\n",
        "                  #              input_tensor[ei], encoder_hidden, torch.Tensor(embedding))\n",
        "                  encoder_output, encoder_hidden = encoder(\n",
        "                                input_tensor[ei], encoder_hidden)\n",
        "                  encoder_outputs[ei] += encoder_output[0, 0]\n",
        "                  last_hidden_state = encoder_outputs[-1].detach()\n",
        "                  xs.append(last_hidden_state.flatten())\n",
        "                  ys.append(duration)\n",
        "            xs = torch.stack(xs)\n",
        "            ys = torch.Tensor(ys).reshape(-1, 1)\n",
        "            loss = criterion(model(xs), ys)\n",
        "\n",
        "            if i % EVERY == 0:\n",
        "                print(f\"Loss: {loss}\")\n",
        "\n",
        "            loss.backward()\n",
        "            b, m = model.parameters()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        print(f\"Final loss on the training data for encoder {ix} is: {loss}\")  # tensor(0.478)\n",
        "        ## TEST ##\n",
        "        test_criterion = torch.nn.MSELoss()\n",
        "        test_xs, test_ys = [], []\n",
        "        for segments, embedding, duration in batch_data:\n",
        "            input_length = len(segments)\n",
        "            encoder_outputs = torch.zeros(input_length, HIDDEN_SIZE,\n",
        "                                          device=DEVICE, requires_grad=False)\n",
        "            if input_length > 0:\n",
        "                input_tensor = tokenize(phone_vocab, segments)\n",
        "                for ei in range(input_length):\n",
        "                    #encoder_output, encoder_hidden = encoder(\n",
        "                    #    input_tensor[ei], encoder_hidden, torch.Tensor(embedding))\n",
        "                    encoder_output, encoder_hidden = encoder(\n",
        "                        input_tensor[ei], encoder_hidden)\n",
        "                    encoder_outputs[ei] += encoder_output[0, 0]\n",
        "                last_hidden_state = encoder_outputs[-1].detach()\n",
        "            test_xs.append(last_hidden_state)\n",
        "            test_ys.append(duration)\n",
        "        test_xs = torch.stack(test_xs)\n",
        "        test_ys = torch.Tensor(test_ys).reshape(-1, 1)\n",
        "        test_loss = test_criterion(model(test_xs), test_ys).detach()\n",
        "        \n",
        "        print(f\"Final loss on the test data for encoder {ix} is: {test_loss}\")  # tensor(0.7235)"
      ],
      "metadata": {
        "id": "X65P0dteVimv"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFjnbHhFViel",
        "outputId": "6f4e9b6f-526a-44f7-940f-647f0a2c44ce"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Encoder Decoder\n",
            "Training model\n",
            "Loss: 63.31273651123047\n",
            "Loss: 2.3395936489105225\n",
            "Loss: 0.48316314816474915\n",
            "Loss: 0.4286659359931946\n",
            "Loss: 1.3541005849838257\n",
            "Loss: 0.8907515406608582\n",
            "Loss: 0.9916452765464783\n",
            "Loss: 0.8088127374649048\n",
            "Loss: 0.8704742193222046\n",
            "Loss: 0.7753809690475464\n",
            "Final loss on the training data for encoder 0 is: 0.5860453248023987\n",
            "Final loss on the test data for encoder 0 is: 0.8913733959197998\n",
            "Loss: 1.1487457752227783\n",
            "Loss: 1.1872951984405518\n",
            "Loss: 0.627408504486084\n",
            "Loss: 1.9141994714736938\n",
            "Loss: 0.5556747913360596\n",
            "Loss: 0.4828616976737976\n",
            "Loss: 0.5209386348724365\n",
            "Loss: 0.5176318287849426\n",
            "Loss: 0.6656030416488647\n",
            "Loss: 0.5227574110031128\n",
            "Final loss on the training data for encoder 1 is: 1.285611867904663\n",
            "Final loss on the test data for encoder 1 is: 1.4258601665496826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-Decoder with CBOW and GLOVE EMbeddings"
      ],
      "metadata": {
        "id": "aMuBjtbxVi9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import pickle, time, math, random, json\n",
        "from train_test_models import BuckeyeDataset, DataLoader\n",
        "import dill\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "OUTPUT_SIZE = 1  # total word duration\n",
        "MAX_LENGTH = 18\n",
        "HIDDEN_SIZE = 8\n",
        "DEVICE = 'cpu'\n",
        "MAX_NORM = 1\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "WEIGHT_DECAY = 0\n",
        "MOMENTUM = 0.0001\n",
        "MAX_LOSS = 1.\n",
        "N_EPOCHS = 500\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "EMBEDDING_SIZE = 8\n",
        "EVERY = 50\n",
        "GLOVE_EMBED_DIM=50\n",
        "\n",
        "EMBEDDING_FILE = './lab_2_data/word2_Vec_Lab_model.pt'\n",
        "PHONE_VOCAB_FILE = './lab_2_data/word2_Vec_Lab_model.vocab'\n",
        "TRAIN_PATH = './data/train.jsonl'\n",
        "TEST_PATH = './data/test.jsonl'\n",
        "VECS_PATH = \"./data/buckeye.vecs\"\n",
        "SHUFFLE_DATA = True\n",
        "\n",
        "def tokenize(vocab, list_of_segments):\n",
        "    return torch.tensor(\n",
        "        [vocab.vocab_to_ix[w] for w in list_of_segments if w in vocab.frequency_table] + [EOS_TOKEN], dtype=torch.long,\n",
        "        device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "def process_segments_for_encoder(line_data):\n",
        "    # Split every line into pairs and normalize\n",
        "    split_str = line_data['observed_pron'].split(\" \")\n",
        "    duration = torch.log(torch.Tensor([sum(line_data['segment_duration_ms'])]))\n",
        "    return (split_str, duration)\n",
        "\n",
        "\n",
        "def tensorFromSentence(vocab, sentence):\n",
        "    indices = tokenize(vocab, sentence)\n",
        "    return indices\n",
        "\n",
        "\n",
        "class Vocab():\n",
        "    def __init__(self, segments: list):\n",
        "        self._compute_frequency_table(segments)\n",
        "        print(self.frequency_table)\n",
        "        self._build_ix_to_vocab_dicts()\n",
        "\n",
        "    def _compute_frequency_table(self, segments):\n",
        "        self.frequency_table = Counter(segments)\n",
        "        self.vocab_size = len(self.frequency_table)\n",
        "\n",
        "    def _build_ix_to_vocab_dicts(self):\n",
        "        self.ix_to_vocab = {\n",
        "            i: phone for i, phone in enumerate(self.frequency_table)\n",
        "            if self.frequency_table[phone] > 0\n",
        "        }\n",
        "        self.vocab_to_ix = {\n",
        "            self.ix_to_vocab[w]: w for w in self.ix_to_vocab.keys()\n",
        "        }\n",
        "\n",
        "    def tokenize(self, list_of_segments):\n",
        "        return torch.tensor(\n",
        "            [self.vocab_to_ix[w] for w in list_of_segments], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "    def detokenize(self, tensor):\n",
        "        return torch.tensor(\n",
        "            [self.ix_to_vocab[ix] for ix in tensor], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "class Word2Vec(torch.nn.Module):\n",
        "    def __init__(self, input_size: int, embedding_size: int, output_size: int = None, max_norm=None):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(\n",
        "            input_size,\n",
        "            embedding_size,\n",
        "            max_norm=MAX_NORM\n",
        "        )\n",
        "        if output_size is None:\n",
        "            self.linear = torch.nn.Linear(embedding_size, input_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(embedding_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.mean(axis=1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class EncoderGloveRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, glove_size=None):\n",
        "        super(EncoderGloveRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        if glove_size is not None:\n",
        "            self.linear = torch.nn.Linear(glove_size, hidden_size)\n",
        "            self.gru = torch.nn.GRU(hidden_size * 2, hidden_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(0, hidden_size)\n",
        "            self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden, glove_embedding=None):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        if glove_embedding is not None:\n",
        "            linear = self.linear(glove_embedding).view(1, 1, -1)\n",
        "            #print(embedded.shape, linear.shape)\n",
        "            output = torch.cat((embedded, linear), axis=2)\n",
        "        else:\n",
        "            output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class EncoderRNN3(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embeddings=None):\n",
        "        super(EncoderRNN3, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        if embeddings is not None:\n",
        "            input_size, hidden_size = embeddings.size()\n",
        "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "            self.embedding.weight = torch.nn.Parameter(embeddings)\n",
        "        else:\n",
        "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class DecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = torch.nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = torch.nn.Dropout(self.dropout_p)\n",
        "        self.gru = torch.nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
        "          max_length=MAX_LENGTH, glove_embedding=None):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max(input_length, max_length), encoder.hidden_size, device=DEVICE)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "      if(type(encoder) is EncoderGloveRNN):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden, glove_embedding)\n",
        "      else:\n",
        "         encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)       \n",
        "      encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_TOKEN]], device=DEVICE)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # use its own predictions as the next input\n",
        "    for di in range(target_length):\n",
        "        if type(decoder) is DecoderRNN:\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "        elif type(decoder) is AttnDecoderRNN:\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "        loss += criterion(decoder_output, target_tensor[di].view(1))\n",
        "        if decoder_input.item() == EOS_TOKEN:\n",
        "            break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return encoder, decoder, loss.item() / target_length\n",
        "\n",
        "\n",
        "def trainIters(pairs, encoder, decoder, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "\n",
        "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "\n",
        "    for training_pair in pairs:\n",
        "        input_tensor, target_tensor, glove_embedding = training_pair\n",
        "\n",
        "        encoder, decoder, loss = train(\n",
        "            input_tensor, target_tensor, encoder,\n",
        "            decoder, encoder_optimizer, decoder_optimizer, criterion, glove_embedding=torch.Tensor(glove_embedding))\n",
        "\n",
        "class LinearModel(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "def main():\n",
        "\n",
        "    #fetching the data\n",
        "    training_data = BuckeyeDataset(TRAIN_PATH, VECS_PATH)\n",
        "    test_data = BuckeyeDataset(TEST_PATH, VECS_PATH)\n",
        "    train_dataloader = DataLoader(\n",
        "        training_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE_DATA, collate_fn=lambda x: x\n",
        "    )\n",
        "    # loading the Vocab class\n",
        "    phone_vocab = dill.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "\n",
        "    # loading learned phone embeddings from CBOW\n",
        "    phone_embeds = Word2Vec(\n",
        "        input_size=phone_vocab.vocab_size,\n",
        "        embedding_size=EMBEDDING_SIZE,\n",
        "        max_norm=MAX_NORM\n",
        "    )\n",
        "    phone_embeds.load_state_dict(torch.load(EMBEDDING_FILE))\n",
        "    embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "\n",
        "    #phone_embeds = torch.load(EMBEDDING_FILE)\n",
        "\n",
        "\n",
        "    #phone_vocab = pickle.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "    #embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "    #evaluating the max length\n",
        "    MAX_LENGTH = max([len(s[0]) for s in training_data])\n",
        "\n",
        "\n",
        "    # Encoder-decoder hidden states\n",
        "    # Encoder-decoder hidden states + learned CBOW embeddings\n",
        "    # Encoder-decoder hidden states + learned phone embeddings + GloVe embeddings\n",
        "    # Encoder-decoder with attention hidden states\n",
        "    # Encoder-decoder with attention hidden states + GloVe embeddings\n",
        "    # Encoder-decoder with attention hidden states + learned CBOW embeddings + GloVe embeddings\n",
        "\n",
        "    n_words_for_encoders = phone_vocab.vocab_size + len([SOS_TOKEN, EOS_TOKEN])\n",
        "    encoder1 = EncoderRNN(n_words_for_encoders, HIDDEN_SIZE).to(DEVICE)\n",
        "    encoder2 = EncoderGloveRNN(n_words_for_encoders, HIDDEN_SIZE, GLOVE_EMBED_DIM).to(DEVICE)\n",
        "    encoder3 = EncoderRNN3(n_words_for_encoders, HIDDEN_SIZE,\n",
        "                           embeddings=torch.nn.Parameter(phone_embeds.embedding.weight)).to(DEVICE)\n",
        "\n",
        "    decoder1 = DecoderRNN(HIDDEN_SIZE, n_words_for_encoders)\n",
        "    attn_decoder1 = AttnDecoderRNN(HIDDEN_SIZE, n_words_for_encoders, dropout_p=0.1).to(DEVICE)\n",
        "    print(\"Training Encoder Decoder\")\n",
        "    for encoder in [encoder1, encoder2, encoder3]:#  , encoder3]: # , encoder2]:\n",
        "        for decoder in [decoder1]: #, attn_decoder1]:\n",
        "            for i in range(N_EPOCHS):\n",
        "                batch_data = next(iter(train_dataloader))\n",
        "                xs, ys, glove_embeddings = [], [], []\n",
        "                for segments, glove_embedding, duration in batch_data:\n",
        "                    if len(segments) > 0:\n",
        "                        xs.append(tensorFromSentence(phone_vocab, segments))\n",
        "                        ys.append(tensorFromSentence(phone_vocab, segments))\n",
        "                        glove_embeddings.append(glove_embedding)\n",
        "                #print(glove_embedding, embedding_dim)\n",
        "                pairs = list(zip(xs, ys, glove_embeddings))\n",
        "                trainIters(pairs, encoder, decoder)\n",
        "                # evaluateRandomly(encoder, decoder, pairs, phone_vocab, n=10)\n",
        "                encoder_type = str(type(encoder)).split(\".\")[-1][:-2]\n",
        "                decoder_type = str(type(decoder)).split(\".\")[-1][:-2]\n",
        "                #torch.save(encoder, f\"./2022-05-11_{type(encoder)}_{type(decoder)}.pt\")\n",
        "                torch.save(encoder, f\"./2022-05-11_{encoder_type}_{decoder_type}.pt\")\n",
        "                #torch.save(encoder.state_dict(), f\"./2022-05-11_{type(encoder)}_{type(decoder)}.pt\")\n",
        "\n",
        "    # train duration prediction model\n",
        "    model = LinearModel(HIDDEN_SIZE, OUTPUT_SIZE)\n",
        "    embedding_matrix = phone_embeds.embedding.weight.detach()\n",
        "    print(\"Training model\")\n",
        "    for ix, encoder in enumerate([encoder1, encoder2, encoder3]):# , encoder3]): #, encoder3]):\n",
        "        encoder.eval()\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        for i in range(N_EPOCHS):\n",
        "            loss = 0\n",
        "            criterion = torch.nn.MSELoss()\n",
        "            #optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "            # make sure we \"zero out\" the loss at each time step\n",
        "            optimizer.zero_grad()\n",
        "            batch_data = next(iter(train_dataloader))\n",
        "            xs, ys = [], []\n",
        "            batch_segments = []\n",
        "            #for idx, batch_data in enumerate(train_dataloader, 0):\n",
        "            for segments, embedding, duration in batch_data:\n",
        "              input_length = len(segments)\n",
        "              encoder_outputs = torch.zeros(input_length, HIDDEN_SIZE,\n",
        "                                              device=DEVICE, requires_grad=False)\n",
        "              if input_length > 0:\n",
        "                input_tensor = tokenize(phone_vocab, segments)\n",
        "                for ei in range(input_length):\n",
        "                  if(type(encoder) is EncoderGloveRNN):\n",
        "                    encoder_output, encoder_hidden = encoder(\n",
        "                      input_tensor[ei], encoder_hidden, torch.Tensor(embedding))\n",
        "                  else:\n",
        "                    encoder_output, encoder_hidden = encoder(\n",
        "                      input_tensor[ei], encoder_hidden)\n",
        "                  encoder_outputs[ei] += encoder_output[0, 0]\n",
        "                  last_hidden_state = encoder_outputs[-1].detach()\n",
        "                  xs.append(last_hidden_state.flatten())\n",
        "                  ys.append(duration)\n",
        "            xs = torch.stack(xs)\n",
        "            ys = torch.Tensor(ys).reshape(-1, 1)\n",
        "            loss = criterion(model(xs), ys)\n",
        "\n",
        "            if i % EVERY == 0:\n",
        "                print(f\"Loss: {loss}\")\n",
        "\n",
        "            loss.backward()\n",
        "            b, m = model.parameters()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        print(f\"Final loss on the training data for encoder {ix} is: {loss}\")  # tensor(0.478)\n",
        "        ## TEST ##\n",
        "        test_criterion = torch.nn.MSELoss()\n",
        "        test_xs, test_ys = [], []\n",
        "        for segments, embedding, duration in batch_data:\n",
        "            input_length = len(segments)\n",
        "            encoder_outputs = torch.zeros(input_length, HIDDEN_SIZE,\n",
        "                                          device=DEVICE, requires_grad=False)\n",
        "            if input_length > 0:\n",
        "                input_tensor = tokenize(phone_vocab, segments)\n",
        "                for ei in range(input_length):\n",
        "                  if(type(encoder) is EncoderGloveRNN):\n",
        "                    encoder_output, encoder_hidden = encoder(\n",
        "                      input_tensor[ei], encoder_hidden, torch.Tensor(embedding))\n",
        "                  else:\n",
        "                    encoder_output, encoder_hidden = encoder(\n",
        "                      input_tensor[ei], encoder_hidden)\n",
        "                    \n",
        "                    encoder_outputs[ei] += encoder_output[0, 0]\n",
        "                last_hidden_state = encoder_outputs[-1].detach()\n",
        "            test_xs.append(last_hidden_state)\n",
        "            test_ys.append(duration)\n",
        "        test_xs = torch.stack(test_xs)\n",
        "        test_ys = torch.Tensor(test_ys).reshape(-1, 1)\n",
        "        test_loss = test_criterion(model(test_xs), test_ys).detach()\n",
        "        \n",
        "        print(f\"Final loss on the test data for encoder {ix} is: {test_loss}\")  # tensor(0.7235)"
      ],
      "metadata": {
        "id": "H885DQwvVm8t"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqgh8kpCVm0a",
        "outputId": "6c887426-00e6-4418-8fea-4d798905e1be"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Encoder Decoder\n",
            "Training model\n",
            "Loss: 65.6651840209961\n",
            "Loss: 0.804783821105957\n",
            "Loss: 0.6187025308609009\n",
            "Loss: 0.6793946623802185\n",
            "Loss: 0.5158143639564514\n",
            "Loss: 0.7093705534934998\n",
            "Loss: 0.9525966048240662\n",
            "Loss: 0.49109992384910583\n",
            "Loss: 0.9505195021629333\n",
            "Loss: 0.5223023295402527\n",
            "Final loss on the training data for encoder 0 is: 0.7255538702011108\n",
            "Final loss on the test data for encoder 0 is: 1.3859784603118896\n",
            "Loss: 2.1690266132354736\n",
            "Loss: 0.3879263401031494\n",
            "Loss: 0.6513210535049438\n",
            "Loss: 0.7673561573028564\n",
            "Loss: 0.6840199828147888\n",
            "Loss: 0.501665472984314\n",
            "Loss: 0.4364526867866516\n",
            "Loss: 0.5918902158737183\n",
            "Loss: 0.4731883406639099\n",
            "Loss: 0.5687894225120544\n",
            "Final loss on the training data for encoder 1 is: 0.5722402930259705\n",
            "Final loss on the test data for encoder 1 is: 40.107059478759766\n",
            "Loss: 7.4033708572387695\n",
            "Loss: 0.4525009095668793\n",
            "Loss: 0.4716859459877014\n",
            "Loss: 0.5113304853439331\n",
            "Loss: 0.6955394744873047\n",
            "Loss: 0.37185657024383545\n",
            "Loss: 0.6209486126899719\n",
            "Loss: 0.474539577960968\n",
            "Loss: 0.45426440238952637\n",
            "Loss: 0.5035009980201721\n",
            "Final loss on the training data for encoder 2 is: 0.3354068398475647\n",
            "Final loss on the test data for encoder 2 is: 0.4166525602340698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-Decoder With Attention"
      ],
      "metadata": {
        "id": "_N5vcB4MQ3wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import pickle, time, math, random, json\n",
        "#from train_test_models import BuckeyeDataset, DataLoader\n",
        "import dill\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "OUTPUT_SIZE = 1  # total word duration\n",
        "MAX_LENGTH = 18\n",
        "HIDDEN_SIZE = 256\n",
        "DEVICE = 'cpu'\n",
        "MAX_NORM = 1\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0\n",
        "MOMENTUM = 0.0001\n",
        "MAX_LOSS = 1.\n",
        "N_EPOCHS = 300\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "EMBEDDING_SIZE = 8\n",
        "EVERY = 30\n",
        "GLOVE_EMBED_DIM=50\n",
        "\n",
        "EMBEDDING_FILE = './lab_2_data/word2_Vec_Lab_model.pt'\n",
        "PHONE_VOCAB_FILE = './lab_2_data/word2_Vec_Lab_model.vocab'\n",
        "TRAIN_PATH = './data/train.jsonl'\n",
        "TEST_PATH = './data/test.jsonl'\n",
        "VECS_PATH = \"./data/buckeye.vecs\"\n",
        "SHUFFLE_DATA = True\n",
        "\n",
        "def tokenize(vocab, list_of_segments):\n",
        "    return torch.tensor(\n",
        "        [vocab.vocab_to_ix[w] for w in list_of_segments if w in vocab.frequency_table] + [EOS_TOKEN], dtype=torch.long,\n",
        "        device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "def process_segments_for_encoder(line_data):\n",
        "    # Split every line into pairs and normalize\n",
        "    split_str = line_data['observed_pron'].split(\" \")\n",
        "    duration = torch.log(torch.Tensor([sum(line_data['segment_duration_ms'])]))\n",
        "    return (split_str, duration)\n",
        "\n",
        "\n",
        "def tensorFromSentence(vocab, sentence):\n",
        "    indices = tokenize(vocab, sentence)\n",
        "    return indices\n",
        "\n",
        "\n",
        "class Vocab():\n",
        "    def __init__(self, segments: list):\n",
        "        self._compute_frequency_table(segments)\n",
        "        print(self.frequency_table)\n",
        "        self._build_ix_to_vocab_dicts()\n",
        "\n",
        "    def _compute_frequency_table(self, segments):\n",
        "        self.frequency_table = Counter(segments)\n",
        "        self.vocab_size = len(self.frequency_table)\n",
        "\n",
        "    def _build_ix_to_vocab_dicts(self):\n",
        "        self.ix_to_vocab = {\n",
        "            i: phone for i, phone in enumerate(self.frequency_table)\n",
        "            if self.frequency_table[phone] > 0\n",
        "        }\n",
        "        self.vocab_to_ix = {\n",
        "            self.ix_to_vocab[w]: w for w in self.ix_to_vocab.keys()\n",
        "        }\n",
        "\n",
        "    def tokenize(self, list_of_segments):\n",
        "        return torch.tensor(\n",
        "            [self.vocab_to_ix[w] for w in list_of_segments], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "    def detokenize(self, tensor):\n",
        "        return torch.tensor(\n",
        "            [self.ix_to_vocab[ix] for ix in tensor], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "class Word2Vec(torch.nn.Module):\n",
        "    def __init__(self, input_size: int, embedding_size: int, output_size: int = None, max_norm=None):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(\n",
        "            input_size,\n",
        "            embedding_size,\n",
        "            max_norm=MAX_NORM\n",
        "        )\n",
        "        if output_size is None:\n",
        "            self.linear = torch.nn.Linear(embedding_size, input_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(embedding_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.mean(axis=1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class EncoderGloveRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, glove_size=None):\n",
        "        super(EncoderGloveRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        if glove_size is not None:\n",
        "            self.linear = torch.nn.Linear(glove_size, hidden_size)\n",
        "            self.gru = torch.nn.GRU(hidden_size * 2, hidden_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(0, hidden_size)\n",
        "            self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden, glove_embedding=None):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        if glove_embedding is not None:\n",
        "            linear = self.linear(glove_embedding).view(1, 1, -1)\n",
        "            #print(embedded.shape, linear.shape)\n",
        "            output = torch.cat((embedded, linear), axis=2)\n",
        "        else:\n",
        "            output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class EncoderRNN3(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embeddings=None):\n",
        "        super(EncoderRNN3, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        if embeddings is not None:\n",
        "            input_size, hidden_size = embeddings.size()\n",
        "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "            self.embedding.weight = torch.nn.Parameter(embeddings)\n",
        "        else:\n",
        "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class DecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = torch.nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = torch.nn.Dropout(self.dropout_p)\n",
        "        self.gru = torch.nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
        "          max_length=MAX_LENGTH, glove_embedding=None):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max(input_length, max_length), encoder.hidden_size, device=DEVICE)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        #encoder_output, encoder_hidden = encoder(\n",
        "        #    input_tensor[ei], encoder_hidden, glove_embedding)\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_TOKEN]], device=DEVICE)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # use its own predictions as the next input\n",
        "    for di in range(target_length):\n",
        "        if type(decoder) is DecoderRNN:\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "        elif type(decoder) is AttnDecoderRNN:\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "        loss += criterion(decoder_output, target_tensor[di].view(1))\n",
        "        if decoder_input.item() == EOS_TOKEN:\n",
        "            break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return encoder, decoder, loss.item() / target_length\n",
        "\n",
        "\n",
        "def trainIters(pairs, encoder, decoder, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "\n",
        "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "\n",
        "    for training_pair in pairs:\n",
        "        #input_tensor, target_tensor, glove_embedding = training_pair\n",
        "        input_tensor, target_tensor = training_pair\n",
        "\n",
        "        #encoder, decoder, loss = train(\n",
        "        #    input_tensor, target_tensor, encoder,\n",
        "        #    decoder, encoder_optimizer, decoder_optimizer, criterion, glove_embedding=torch.Tensor(glove_embedding))\n",
        "        encoder, decoder, loss = train(\n",
        "            input_tensor, target_tensor, encoder,\n",
        "            decoder, encoder_optimizer, decoder_optimizer, criterion,)\n",
        "\n",
        "\n",
        "class LinearModel(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "def main():\n",
        "\n",
        "    #fetching the data\n",
        "    training_data = BuckeyeDataset(TRAIN_PATH, VECS_PATH)\n",
        "    test_data = BuckeyeDataset(TEST_PATH, VECS_PATH)\n",
        "    train_dataloader = DataLoader(\n",
        "        training_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE_DATA, collate_fn=lambda x: x\n",
        "    )\n",
        "    # loading the Vocab class\n",
        "    phone_vocab = dill.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "\n",
        "    # loading learned phone embeddings from CBOW\n",
        "    phone_embeds = Word2Vec(\n",
        "        input_size=phone_vocab.vocab_size,\n",
        "        embedding_size=EMBEDDING_SIZE,\n",
        "        max_norm=MAX_NORM\n",
        "    )\n",
        "    phone_embeds.load_state_dict(torch.load(EMBEDDING_FILE))\n",
        "    embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "\n",
        "    #phone_embeds = torch.load(EMBEDDING_FILE)\n",
        "\n",
        "\n",
        "    #phone_vocab = pickle.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "    #embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "    #evaluating the max length\n",
        "    MAX_LENGTH = max([len(s[0]) for s in training_data])\n",
        "\n",
        "\n",
        "    # Encoder-decoder hidden states\n",
        "    # Encoder-decoder hidden states + learned CBOW embeddings\n",
        "    # Encoder-decoder hidden states + learned phone embeddings + GloVe embeddings\n",
        "    # Encoder-decoder with attention hidden states\n",
        "    # Encoder-decoder with attention hidden states + GloVe embeddings\n",
        "    # Encoder-decoder with attention hidden states + learned CBOW embeddings + GloVe embeddings\n",
        "\n",
        "    n_words_for_encoders = phone_vocab.vocab_size + len([SOS_TOKEN, EOS_TOKEN])\n",
        "    encoder1 = EncoderRNN(n_words_for_encoders, HIDDEN_SIZE).to(DEVICE)\n",
        "    encoder2 = EncoderGloveRNN(n_words_for_encoders, HIDDEN_SIZE, GLOVE_EMBED_DIM).to(DEVICE)\n",
        "    encoder3 = EncoderRNN3(n_words_for_encoders, HIDDEN_SIZE,\n",
        "                           embeddings=torch.nn.Parameter(phone_embeds.embedding.weight)).to(DEVICE)\n",
        "\n",
        "    decoder1 = DecoderRNN(HIDDEN_SIZE, n_words_for_encoders)\n",
        "    attn_decoder1 = AttnDecoderRNN(HIDDEN_SIZE, n_words_for_encoders, dropout_p=0.1).to(DEVICE)\n",
        "    print(\"Training Encoder Decoder\")\n",
        "    for encoder in [encoder1]:#  , encoder3]: # , encoder2]:\n",
        "        for decoder in [attn_decoder1]: #, attn_decoder1]:\n",
        "            for i in range(N_EPOCHS):\n",
        "                batch_data = next(iter(train_dataloader))\n",
        "                xs, ys, glove_embeddings = [], [], []\n",
        "                for segments, glove_embedding, duration in batch_data:\n",
        "                    if len(segments) > 0:\n",
        "                        xs.append(tensorFromSentence(phone_vocab, segments))\n",
        "                        ys.append(tensorFromSentence(phone_vocab, segments))\n",
        "                        glove_embeddings.append(glove_embedding)\n",
        "                #print(glove_embedding, embedding_dim)\n",
        "                #pairs = list(zip(xs, ys, glove_embeddings))\n",
        "                pairs = list(zip(xs, ys))\n",
        "                trainIters(pairs, encoder, decoder)\n",
        "                # evaluateRandomly(encoder, decoder, pairs, phone_vocab, n=10)\n",
        "                encoder_type = str(type(encoder)).split(\".\")[-1][:-2]\n",
        "                decoder_type = str(type(decoder)).split(\".\")[-1][:-2]\n",
        "                #torch.save(encoder, f\"./2022-05-11_{type(encoder)}_{type(decoder)}.pt\")\n",
        "                torch.save(encoder, f\"./2022-05-11_{encoder_type}_{decoder_type}.pt\")\n",
        "                #torch.save(encoder.state_dict(), f\"./2022-05-11_{type(encoder)}_{type(decoder)}.pt\")\n",
        "\n",
        "    # train duration prediction model\n",
        "    model = LinearModel(HIDDEN_SIZE, OUTPUT_SIZE)\n",
        "    embedding_matrix = phone_embeds.embedding.weight.detach()\n",
        "    print(\"Training model\")\n",
        "    for ix, encoder in enumerate([encoder1]):# , encoder3]): #, encoder3]):\n",
        "        encoder.eval()\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        for i in range(N_EPOCHS):\n",
        "            loss = 0\n",
        "            criterion = torch.nn.MSELoss()\n",
        "            #optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "            # make sure we \"zero out\" the loss at each time step\n",
        "            optimizer.zero_grad()\n",
        "            batch_data = next(iter(train_dataloader))\n",
        "            xs, ys = [], []\n",
        "            batch_segments = []\n",
        "            #for idx, batch_data in enumerate(train_dataloader, 0):\n",
        "            for segments, embedding, duration in batch_data:\n",
        "              input_length = len(segments)\n",
        "              encoder_outputs = torch.zeros(input_length, HIDDEN_SIZE,\n",
        "                                              device=DEVICE, requires_grad=False)\n",
        "              if input_length > 0:\n",
        "                input_tensor = tokenize(phone_vocab, segments)\n",
        "                for ei in range(input_length):\n",
        "                  #encoder_output, encoder_hidden = encoder(\n",
        "                  #              input_tensor[ei], encoder_hidden, torch.Tensor(embedding))\n",
        "                  encoder_output, encoder_hidden = encoder(\n",
        "                                input_tensor[ei], encoder_hidden)\n",
        "                  encoder_outputs[ei] += encoder_output[0, 0]\n",
        "                  last_hidden_state = encoder_outputs[-1].detach()\n",
        "                  xs.append(last_hidden_state.flatten())\n",
        "                  ys.append(duration)\n",
        "            xs = torch.stack(xs)\n",
        "            ys = torch.Tensor(ys).reshape(-1, 1)\n",
        "            loss = criterion(model(xs), ys)\n",
        "\n",
        "            if i % EVERY == 0:\n",
        "                print(f\"Loss: {loss}\")\n",
        "\n",
        "            loss.backward()\n",
        "            b, m = model.parameters()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        print(f\"Final loss on the training data for encoder {ix} is: {loss}\")  # tensor(0.478)\n",
        "        ## TEST ##\n",
        "        test_criterion = torch.nn.MSELoss()\n",
        "        test_xs, test_ys = [], []\n",
        "        for segments, embedding, duration in batch_data:\n",
        "            input_length = len(segments)\n",
        "            encoder_outputs = torch.zeros(input_length, HIDDEN_SIZE,\n",
        "                                          device=DEVICE, requires_grad=False)\n",
        "            if input_length > 0:\n",
        "                input_tensor = tokenize(phone_vocab, segments)\n",
        "                for ei in range(input_length):\n",
        "                    #encoder_output, encoder_hidden = encoder(\n",
        "                    #    input_tensor[ei], encoder_hidden, torch.Tensor(embedding))\n",
        "                    encoder_output, encoder_hidden = encoder(\n",
        "                        input_tensor[ei], encoder_hidden)\n",
        "                    encoder_outputs[ei] += encoder_output[0, 0]\n",
        "                last_hidden_state = encoder_outputs[-1].detach()\n",
        "            test_xs.append(last_hidden_state)\n",
        "            test_ys.append(duration)\n",
        "        test_xs = torch.stack(test_xs)\n",
        "        test_ys = torch.Tensor(test_ys).reshape(-1, 1)\n",
        "        test_loss = test_criterion(model(test_xs), test_ys).detach()\n",
        "        \n",
        "        print(f\"Final loss on the test data for encoder {ix} is: {test_loss}\")  # tensor(0.7235)"
      ],
      "metadata": {
        "id": "37ZrfwokQwfQ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzcObUKrRfgH",
        "outputId": "bf3b898f-3d29-4cb2-91c2-fd6626cb0972"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Encoder Decoder\n",
            "Training model\n",
            "Loss: 57.8001594543457\n",
            "Loss: 0.9664241075515747\n",
            "Loss: 0.6025242209434509\n",
            "Loss: 0.6730889678001404\n",
            "Loss: 0.619820237159729\n",
            "Loss: 0.573050856590271\n",
            "Loss: 0.43242523074150085\n",
            "Loss: 0.5873600840568542\n",
            "Loss: 0.5564441680908203\n",
            "Loss: 0.5288306474685669\n",
            "Final loss on the training data for encoder 0 is: 0.5834252834320068\n",
            "Final loss on the test data for encoder 0 is: 0.8446372747421265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-Decoder With Attention and CBOW Embeddings\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JSmwjiQsTws3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import pickle, time, math, random, json\n",
        "#from train_test_models import BuckeyeDataset, DataLoader\n",
        "import dill\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "OUTPUT_SIZE = 1  # total word duration\n",
        "MAX_LENGTH = 18\n",
        "HIDDEN_SIZE = 8\n",
        "DEVICE = 'cpu'\n",
        "MAX_NORM = 1\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "WEIGHT_DECAY = 0\n",
        "MOMENTUM = 0.001\n",
        "MAX_LOSS = 1.\n",
        "N_EPOCHS = 300\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "EMBEDDING_SIZE = 8\n",
        "EVERY = 30\n",
        "GLOVE_EMBED_DIM=50\n",
        "\n",
        "EMBEDDING_FILE = './lab_2_data/word2_Vec_Lab_model.pt'\n",
        "PHONE_VOCAB_FILE = './lab_2_data/word2_Vec_Lab_model.vocab'\n",
        "TRAIN_PATH = './data/train.jsonl'\n",
        "TEST_PATH = './data/test.jsonl'\n",
        "VECS_PATH = \"./data/buckeye.vecs\"\n",
        "SHUFFLE_DATA = True\n",
        "\n",
        "def tokenize(vocab, list_of_segments):\n",
        "    return torch.tensor(\n",
        "        [vocab.vocab_to_ix[w] for w in list_of_segments if w in vocab.frequency_table] + [EOS_TOKEN], dtype=torch.long,\n",
        "        device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "def process_segments_for_encoder(line_data):\n",
        "    # Split every line into pairs and normalize\n",
        "    split_str = line_data['observed_pron'].split(\" \")\n",
        "    duration = torch.log(torch.Tensor([sum(line_data['segment_duration_ms'])]))\n",
        "    return (split_str, duration)\n",
        "\n",
        "\n",
        "def tensorFromSentence(vocab, sentence):\n",
        "    indices = tokenize(vocab, sentence)\n",
        "    return indices\n",
        "\n",
        "\n",
        "class Vocab():\n",
        "    def __init__(self, segments: list):\n",
        "        self._compute_frequency_table(segments)\n",
        "        print(self.frequency_table)\n",
        "        self._build_ix_to_vocab_dicts()\n",
        "\n",
        "    def _compute_frequency_table(self, segments):\n",
        "        self.frequency_table = Counter(segments)\n",
        "        self.vocab_size = len(self.frequency_table)\n",
        "\n",
        "    def _build_ix_to_vocab_dicts(self):\n",
        "        self.ix_to_vocab = {\n",
        "            i: phone for i, phone in enumerate(self.frequency_table)\n",
        "            if self.frequency_table[phone] > 0\n",
        "        }\n",
        "        self.vocab_to_ix = {\n",
        "            self.ix_to_vocab[w]: w for w in self.ix_to_vocab.keys()\n",
        "        }\n",
        "\n",
        "    def tokenize(self, list_of_segments):\n",
        "        return torch.tensor(\n",
        "            [self.vocab_to_ix[w] for w in list_of_segments], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "    def detokenize(self, tensor):\n",
        "        return torch.tensor(\n",
        "            [self.ix_to_vocab[ix] for ix in tensor], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "class Word2Vec(torch.nn.Module):\n",
        "    def __init__(self, input_size: int, embedding_size: int, output_size: int = None, max_norm=None):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(\n",
        "            input_size,\n",
        "            embedding_size,\n",
        "            max_norm=MAX_NORM\n",
        "        )\n",
        "        if output_size is None:\n",
        "            self.linear = torch.nn.Linear(embedding_size, input_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(embedding_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.mean(axis=1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class EncoderGloveRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, glove_size=None):\n",
        "        super(EncoderGloveRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        if glove_size is not None:\n",
        "            self.linear = torch.nn.Linear(glove_size, hidden_size)\n",
        "            self.gru = torch.nn.GRU(hidden_size * 2, hidden_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(0, hidden_size)\n",
        "            self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden, glove_embedding=None):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        if glove_embedding is not None:\n",
        "            linear = self.linear(glove_embedding).view(1, 1, -1)\n",
        "            #print(embedded.shape, linear.shape)\n",
        "            output = torch.cat((embedded, linear), axis=2)\n",
        "        else:\n",
        "            output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class EncoderRNN3(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embeddings=None):\n",
        "        super(EncoderRNN3, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        if embeddings is not None:\n",
        "            input_size, hidden_size = embeddings.size()\n",
        "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "            self.embedding.weight = torch.nn.Parameter(embeddings)\n",
        "        else:\n",
        "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class DecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = torch.nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = torch.nn.Dropout(self.dropout_p)\n",
        "        self.gru = torch.nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
        "          max_length=MAX_LENGTH, glove_embedding=None):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max(input_length, max_length), encoder.hidden_size, device=DEVICE)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        #encoder_output, encoder_hidden = encoder(\n",
        "        #    input_tensor[ei], encoder_hidden, glove_embedding)\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_TOKEN]], device=DEVICE)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # use its own predictions as the next input\n",
        "    for di in range(target_length):\n",
        "        if type(decoder) is DecoderRNN:\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "        elif type(decoder) is AttnDecoderRNN:\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "        loss += criterion(decoder_output, target_tensor[di].view(1))\n",
        "        if decoder_input.item() == EOS_TOKEN:\n",
        "            break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return encoder, decoder, loss.item() / target_length\n",
        "\n",
        "\n",
        "def trainIters(pairs, encoder, decoder, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "\n",
        "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "\n",
        "    for training_pair in pairs:\n",
        "        #input_tensor, target_tensor, glove_embedding = training_pair\n",
        "        input_tensor, target_tensor = training_pair\n",
        "\n",
        "        #encoder, decoder, loss = train(\n",
        "        #    input_tensor, target_tensor, encoder,\n",
        "        #    decoder, encoder_optimizer, decoder_optimizer, criterion, glove_embedding=torch.Tensor(glove_embedding))\n",
        "        encoder, decoder, loss = train(\n",
        "            input_tensor, target_tensor, encoder,\n",
        "            decoder, encoder_optimizer, decoder_optimizer, criterion,)\n",
        "\n",
        "\n",
        "class LinearModel(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "def main():\n",
        "\n",
        "    #fetching the data\n",
        "    training_data = BuckeyeDataset(TRAIN_PATH, VECS_PATH)\n",
        "    test_data = BuckeyeDataset(TEST_PATH, VECS_PATH)\n",
        "    train_dataloader = DataLoader(\n",
        "        training_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE_DATA, collate_fn=lambda x: x\n",
        "    )\n",
        "    # loading the Vocab class\n",
        "    phone_vocab = dill.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "\n",
        "    # loading learned phone embeddings from CBOW\n",
        "    phone_embeds = Word2Vec(\n",
        "        input_size=phone_vocab.vocab_size,\n",
        "        embedding_size=EMBEDDING_SIZE,\n",
        "        max_norm=MAX_NORM\n",
        "    )\n",
        "    phone_embeds.load_state_dict(torch.load(EMBEDDING_FILE))\n",
        "    embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "\n",
        "    #phone_embeds = torch.load(EMBEDDING_FILE)\n",
        "\n",
        "\n",
        "    #phone_vocab = pickle.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "    #embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "    #evaluating the max length\n",
        "    MAX_LENGTH = max([len(s[0]) for s in training_data])\n",
        "\n",
        "\n",
        "    # Encoder-decoder hidden states\n",
        "    # Encoder-decoder hidden states + learned CBOW embeddings\n",
        "    # Encoder-decoder hidden states + learned phone embeddings + GloVe embeddings\n",
        "    # Encoder-decoder with attention hidden states\n",
        "    # Encoder-decoder with attention hidden states + GloVe embeddings\n",
        "    # Encoder-decoder with attention hidden states + learned CBOW embeddings + GloVe embeddings\n",
        "\n",
        "    n_words_for_encoders = phone_vocab.vocab_size + len([SOS_TOKEN, EOS_TOKEN])\n",
        "    encoder1 = EncoderRNN(n_words_for_encoders, HIDDEN_SIZE).to(DEVICE)\n",
        "    encoder2 = EncoderGloveRNN(n_words_for_encoders, HIDDEN_SIZE, GLOVE_EMBED_DIM).to(DEVICE)\n",
        "    encoder3 = EncoderRNN3(n_words_for_encoders, HIDDEN_SIZE,\n",
        "                           embeddings=torch.nn.Parameter(phone_embeds.embedding.weight)).to(DEVICE)\n",
        "\n",
        "    decoder1 = DecoderRNN(HIDDEN_SIZE, n_words_for_encoders)\n",
        "    attn_decoder1 = AttnDecoderRNN(HIDDEN_SIZE, n_words_for_encoders, dropout_p=0.1).to(DEVICE)\n",
        "    print(\"Training Encoder Decoder\")\n",
        "    for encoder in [encoder1, encoder3]:#  , encoder3]: # , encoder2]:\n",
        "        for decoder in [attn_decoder1]: #, attn_decoder1]:\n",
        "            for i in range(N_EPOCHS):\n",
        "                batch_data = next(iter(train_dataloader))\n",
        "                xs, ys, glove_embeddings = [], [], []\n",
        "                for segments, glove_embedding, duration in batch_data:\n",
        "                    if len(segments) > 0:\n",
        "                        xs.append(tensorFromSentence(phone_vocab, segments))\n",
        "                        ys.append(tensorFromSentence(phone_vocab, segments))\n",
        "                        glove_embeddings.append(glove_embedding)\n",
        "                #print(glove_embedding, embedding_dim)\n",
        "                #pairs = list(zip(xs, ys, glove_embeddings))\n",
        "                pairs = list(zip(xs, ys))\n",
        "                trainIters(pairs, encoder, decoder)\n",
        "                # evaluateRandomly(encoder, decoder, pairs, phone_vocab, n=10)\n",
        "                encoder_type = str(type(encoder)).split(\".\")[-1][:-2]\n",
        "                decoder_type = str(type(decoder)).split(\".\")[-1][:-2]\n",
        "                #torch.save(encoder, f\"./2022-05-11_{type(encoder)}_{type(decoder)}.pt\")\n",
        "                torch.save(encoder, f\"./2022-05-11_{encoder_type}_{decoder_type}.pt\")\n",
        "                #torch.save(encoder.state_dict(), f\"./2022-05-11_{type(encoder)}_{type(decoder)}.pt\")\n",
        "\n",
        "    # train duration prediction model\n",
        "    model = LinearModel(HIDDEN_SIZE, OUTPUT_SIZE)\n",
        "    embedding_matrix = phone_embeds.embedding.weight.detach()\n",
        "    print(\"Training model\")\n",
        "    for ix, encoder in enumerate([encoder1, encoder3]):# , encoder3]): #, encoder3]):\n",
        "        encoder.eval()\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        for i in range(N_EPOCHS):\n",
        "            loss = 0\n",
        "            criterion = torch.nn.MSELoss()\n",
        "            #optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "            # make sure we \"zero out\" the loss at each time step\n",
        "            optimizer.zero_grad()\n",
        "            batch_data = next(iter(train_dataloader))\n",
        "            xs, ys = [], []\n",
        "            batch_segments = []\n",
        "            #for idx, batch_data in enumerate(train_dataloader, 0):\n",
        "            for segments, embedding, duration in batch_data:\n",
        "              input_length = len(segments)\n",
        "              encoder_outputs = torch.zeros(input_length, HIDDEN_SIZE,\n",
        "                                              device=DEVICE, requires_grad=False)\n",
        "              if input_length > 0:\n",
        "                input_tensor = tokenize(phone_vocab, segments)\n",
        "                for ei in range(input_length):\n",
        "                  #encoder_output, encoder_hidden = encoder(\n",
        "                  #              input_tensor[ei], encoder_hidden, torch.Tensor(embedding))\n",
        "                  encoder_output, encoder_hidden = encoder(\n",
        "                                input_tensor[ei], encoder_hidden)\n",
        "                  encoder_outputs[ei] += encoder_output[0, 0]\n",
        "                  last_hidden_state = encoder_outputs[-1].detach()\n",
        "                  xs.append(last_hidden_state.flatten())\n",
        "                  ys.append(duration)\n",
        "            xs = torch.stack(xs)\n",
        "            ys = torch.Tensor(ys).reshape(-1, 1)\n",
        "            loss = criterion(model(xs), ys)\n",
        "\n",
        "            if i % EVERY == 0:\n",
        "                print(f\"Loss: {loss}\")\n",
        "\n",
        "            loss.backward()\n",
        "            b, m = model.parameters()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        print(f\"Final loss on the training data for encoder {ix} is: {loss}\")  # tensor(0.478)\n",
        "        ## TEST ##\n",
        "        test_criterion = torch.nn.MSELoss()\n",
        "        test_xs, test_ys = [], []\n",
        "        for segments, embedding, duration in batch_data:\n",
        "            input_length = len(segments)\n",
        "            encoder_outputs = torch.zeros(input_length, HIDDEN_SIZE,\n",
        "                                          device=DEVICE, requires_grad=False)\n",
        "            if input_length > 0:\n",
        "                input_tensor = tokenize(phone_vocab, segments)\n",
        "                for ei in range(input_length):\n",
        "                    #encoder_output, encoder_hidden = encoder(\n",
        "                    #    input_tensor[ei], encoder_hidden, torch.Tensor(embedding))\n",
        "                    encoder_output, encoder_hidden = encoder(\n",
        "                        input_tensor[ei], encoder_hidden)\n",
        "                    encoder_outputs[ei] += encoder_output[0, 0]\n",
        "                last_hidden_state = encoder_outputs[-1].detach()\n",
        "            test_xs.append(last_hidden_state)\n",
        "            test_ys.append(duration)\n",
        "        test_xs = torch.stack(test_xs)\n",
        "        test_ys = torch.Tensor(test_ys).reshape(-1, 1)\n",
        "        test_loss = test_criterion(model(test_xs), test_ys).detach()\n",
        "        \n",
        "        print(f\"Final loss on the test data for encoder {ix} is: {test_loss}\")  # tensor(0.7235)"
      ],
      "metadata": {
        "id": "tB5Xl_3NRq4O"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s9wYTIFUb6T",
        "outputId": "1d3d32c8-54d5-4dfe-d836-2bb610b15c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Encoder Decoder\n",
            "Training model\n",
            "Loss: 55.89055252075195\n",
            "Loss: 0.8378207087516785\n",
            "Loss: 0.6185505986213684\n",
            "Loss: 0.691376268863678\n",
            "Loss: 0.5926519632339478\n",
            "Loss: 0.8279205560684204\n",
            "Loss: 0.9110522270202637\n",
            "Loss: 0.4455769658088684\n",
            "Loss: 0.8437288403511047\n",
            "Final loss on the training data for encoder 0 is: 0.4587155878543854\n",
            "Final loss on the test data for encoder 0 is: 0.48783427476882935\n",
            "Loss: 22.000808715820312\n",
            "Loss: 1.621963381767273\n",
            "Loss: 0.8436983823776245\n",
            "Loss: 0.9428918957710266\n",
            "Loss: 1.2026623487472534\n",
            "Loss: 0.8381234407424927\n",
            "Loss: 0.42408397793769836\n",
            "Loss: 0.7704006433486938\n",
            "Loss: 1.0019842386245728\n",
            "Loss: 0.9775902032852173\n",
            "Final loss on the training data for encoder 1 is: 0.6189470291137695\n",
            "Final loss on the test data for encoder 1 is: 0.9605862498283386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-Decoder With Attention and CBOW, GLOVE Embeddings"
      ],
      "metadata": {
        "id": "l-IoCL0KUkdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import pickle, time, math, random, json\n",
        "#from train_test_models import BuckeyeDataset, DataLoader\n",
        "import dill\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "OUTPUT_SIZE = 1  # total word duration\n",
        "MAX_LENGTH = 18\n",
        "HIDDEN_SIZE = 8\n",
        "DEVICE = 'cpu'\n",
        "MAX_NORM = 1\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "WEIGHT_DECAY = 0\n",
        "MOMENTUM = 0.0001\n",
        "MAX_LOSS = 1.\n",
        "N_EPOCHS = 500\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE = True\n",
        "EMBEDDING_SIZE = 8\n",
        "EVERY = 50\n",
        "GLOVE_EMBED_DIM=50\n",
        "\n",
        "EMBEDDING_FILE = './lab_2_data/word2_Vec_Lab_model.pt'\n",
        "PHONE_VOCAB_FILE = './lab_2_data/word2_Vec_Lab_model.vocab'\n",
        "TRAIN_PATH = './data/train.jsonl'\n",
        "TEST_PATH = './data/test.jsonl'\n",
        "VECS_PATH = \"./data/buckeye.vecs\"\n",
        "SHUFFLE_DATA = True\n",
        "\n",
        "def tokenize(vocab, list_of_segments):\n",
        "    return torch.tensor(\n",
        "        [vocab.vocab_to_ix[w] for w in list_of_segments if w in vocab.frequency_table] + [EOS_TOKEN], dtype=torch.long,\n",
        "        device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "def process_segments_for_encoder(line_data):\n",
        "    # Split every line into pairs and normalize\n",
        "    split_str = line_data['observed_pron'].split(\" \")\n",
        "    duration = torch.log(torch.Tensor([sum(line_data['segment_duration_ms'])]))\n",
        "    return (split_str, duration)\n",
        "\n",
        "\n",
        "def tensorFromSentence(vocab, sentence):\n",
        "    indices = tokenize(vocab, sentence)\n",
        "    return indices\n",
        "\n",
        "\n",
        "class Vocab():\n",
        "    def __init__(self, segments: list):\n",
        "        self._compute_frequency_table(segments)\n",
        "        print(self.frequency_table)\n",
        "        self._build_ix_to_vocab_dicts()\n",
        "\n",
        "    def _compute_frequency_table(self, segments):\n",
        "        self.frequency_table = Counter(segments)\n",
        "        self.vocab_size = len(self.frequency_table)\n",
        "\n",
        "    def _build_ix_to_vocab_dicts(self):\n",
        "        self.ix_to_vocab = {\n",
        "            i: phone for i, phone in enumerate(self.frequency_table)\n",
        "            if self.frequency_table[phone] > 0\n",
        "        }\n",
        "        self.vocab_to_ix = {\n",
        "            self.ix_to_vocab[w]: w for w in self.ix_to_vocab.keys()\n",
        "        }\n",
        "\n",
        "    def tokenize(self, list_of_segments):\n",
        "        return torch.tensor(\n",
        "            [self.vocab_to_ix[w] for w in list_of_segments], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "    def detokenize(self, tensor):\n",
        "        return torch.tensor(\n",
        "            [self.ix_to_vocab[ix] for ix in tensor], dtype=torch.long,\n",
        "            device=DEVICE).view(-1, 1)\n",
        "\n",
        "\n",
        "class Word2Vec(torch.nn.Module):\n",
        "    def __init__(self, input_size: int, embedding_size: int, output_size: int = None, max_norm=None):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(\n",
        "            input_size,\n",
        "            embedding_size,\n",
        "            max_norm=MAX_NORM\n",
        "        )\n",
        "        if output_size is None:\n",
        "            self.linear = torch.nn.Linear(embedding_size, input_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(embedding_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.mean(axis=1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class EncoderGloveRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, glove_size=None):\n",
        "        super(EncoderGloveRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        if glove_size is not None:\n",
        "            self.linear = torch.nn.Linear(glove_size, hidden_size)\n",
        "            self.gru = torch.nn.GRU(hidden_size * 2, hidden_size)\n",
        "        else:\n",
        "            self.linear = torch.nn.Linear(0, hidden_size)\n",
        "            self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden, glove_embedding=None):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        if glove_embedding is not None:\n",
        "            linear = self.linear(glove_embedding).view(1, 1, -1)\n",
        "            #print(embedded.shape, linear.shape)\n",
        "            output = torch.cat((embedded, linear), axis=2)\n",
        "        else:\n",
        "            output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class EncoderRNN3(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embeddings=None):\n",
        "        super(EncoderRNN3, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        if embeddings is not None:\n",
        "            input_size, hidden_size = embeddings.size()\n",
        "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "            self.embedding.weight = torch.nn.Parameter(embeddings)\n",
        "        else:\n",
        "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class DecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = torch.nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = torch.nn.Dropout(self.dropout_p)\n",
        "        self.gru = torch.nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
        "          max_length=MAX_LENGTH, glove_embedding=None):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max(input_length, max_length), encoder.hidden_size, device=DEVICE)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "      if(type(encoder) is EncoderGloveRNN):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden, glove_embedding)\n",
        "      else:\n",
        "         encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)       \n",
        "      encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_TOKEN]], device=DEVICE)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # use its own predictions as the next input\n",
        "    for di in range(target_length):\n",
        "        if type(decoder) is DecoderRNN:\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "        elif type(decoder) is AttnDecoderRNN:\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "        loss += criterion(decoder_output, target_tensor[di].view(1))\n",
        "        if decoder_input.item() == EOS_TOKEN:\n",
        "            break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return encoder, decoder, loss.item() / target_length\n",
        "\n",
        "\n",
        "def trainIters(pairs, encoder, decoder, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "\n",
        "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "\n",
        "    for training_pair in pairs:\n",
        "        input_tensor, target_tensor, glove_embedding = training_pair\n",
        "        #input_tensor, target_tensor = training_pair\n",
        "\n",
        "        encoder, decoder, loss = train(\n",
        "            input_tensor, target_tensor, encoder,\n",
        "            decoder, encoder_optimizer, decoder_optimizer, criterion, glove_embedding=torch.Tensor(glove_embedding))\n",
        "        #encoder, decoder, loss = train(\n",
        "        #    input_tensor, target_tensor, encoder,\n",
        "        #    decoder, encoder_optimizer, decoder_optimizer, criterion,)\n",
        "\n",
        "\n",
        "class LinearModel(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "def main():\n",
        "\n",
        "    #fetching the data\n",
        "    training_data = BuckeyeDataset(TRAIN_PATH, VECS_PATH)\n",
        "    test_data = BuckeyeDataset(TEST_PATH, VECS_PATH)\n",
        "    train_dataloader = DataLoader(\n",
        "        training_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE_DATA, collate_fn=lambda x: x\n",
        "    )\n",
        "    # loading the Vocab class\n",
        "    phone_vocab = dill.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "\n",
        "    # loading learned phone embeddings from CBOW\n",
        "    phone_embeds = Word2Vec(\n",
        "        input_size=phone_vocab.vocab_size,\n",
        "        embedding_size=EMBEDDING_SIZE,\n",
        "        max_norm=MAX_NORM\n",
        "    )\n",
        "    phone_embeds.load_state_dict(torch.load(EMBEDDING_FILE))\n",
        "    embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "\n",
        "    #phone_embeds = torch.load(EMBEDDING_FILE)\n",
        "\n",
        "\n",
        "    #phone_vocab = pickle.load(open(PHONE_VOCAB_FILE, 'rb'))\n",
        "    #embedding_dim = phone_embeds.embedding.weight.size()[-1]\n",
        "\n",
        "    #evaluating the max length\n",
        "    MAX_LENGTH = max([len(s[0]) for s in training_data])\n",
        "\n",
        "\n",
        "    # Encoder-decoder hidden states\n",
        "    # Encoder-decoder hidden states + learned CBOW embeddings\n",
        "    # Encoder-decoder hidden states + learned phone embeddings + GloVe embeddings\n",
        "    # Encoder-decoder with attention hidden states\n",
        "    # Encoder-decoder with attention hidden states + GloVe embeddings\n",
        "    # Encoder-decoder with attention hidden states + learned CBOW embeddings + GloVe embeddings\n",
        "\n",
        "    n_words_for_encoders = phone_vocab.vocab_size + len([SOS_TOKEN, EOS_TOKEN])\n",
        "    encoder1 = EncoderRNN(n_words_for_encoders, HIDDEN_SIZE).to(DEVICE)\n",
        "    encoder2 = EncoderGloveRNN(n_words_for_encoders, HIDDEN_SIZE, GLOVE_EMBED_DIM).to(DEVICE)\n",
        "    encoder3 = EncoderRNN3(n_words_for_encoders, HIDDEN_SIZE,\n",
        "                           embeddings=torch.nn.Parameter(phone_embeds.embedding.weight)).to(DEVICE)\n",
        "\n",
        "    decoder1 = DecoderRNN(HIDDEN_SIZE, n_words_for_encoders)\n",
        "    attn_decoder1 = AttnDecoderRNN(HIDDEN_SIZE, n_words_for_encoders, dropout_p=0.1).to(DEVICE)\n",
        "    print(\"Training Encoder Decoder\")\n",
        "    for encoder in [encoder1, encoder2, encoder3]:#  , encoder3]: # , encoder2]:\n",
        "        for decoder in [attn_decoder1]: #, attn_decoder1]:\n",
        "            for i in range(N_EPOCHS):\n",
        "                batch_data = next(iter(train_dataloader))\n",
        "                xs, ys, glove_embeddings = [], [], []\n",
        "                for segments, glove_embedding, duration in batch_data:\n",
        "                    if len(segments) > 0:\n",
        "                        xs.append(tensorFromSentence(phone_vocab, segments))\n",
        "                        ys.append(tensorFromSentence(phone_vocab, segments))\n",
        "                        glove_embeddings.append(glove_embedding)\n",
        "                #print(glove_embedding, embedding_dim)\n",
        "                pairs = list(zip(xs, ys, glove_embeddings))\n",
        "                #pairs = list(zip(xs, ys))\n",
        "                trainIters(pairs, encoder, decoder)\n",
        "                # evaluateRandomly(encoder, decoder, pairs, phone_vocab, n=10)\n",
        "                encoder_type = str(type(encoder)).split(\".\")[-1][:-2]\n",
        "                decoder_type = str(type(decoder)).split(\".\")[-1][:-2]\n",
        "                #torch.save(encoder, f\"./2022-05-11_{type(encoder)}_{type(decoder)}.pt\")\n",
        "                torch.save(encoder, f\"./2022-05-11_{encoder_type}_{decoder_type}.pt\")\n",
        "                #torch.save(encoder.state_dict(), f\"./2022-05-11_{type(encoder)}_{type(decoder)}.pt\")\n",
        "\n",
        "    # train duration prediction model\n",
        "    model = LinearModel(HIDDEN_SIZE, OUTPUT_SIZE)\n",
        "    embedding_matrix = phone_embeds.embedding.weight.detach()\n",
        "    print(\"Training model\")\n",
        "    for ix, encoder in enumerate([encoder1, encoder2, encoder3]):# , encoder3]): #, encoder3]):\n",
        "        encoder.eval()\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        for i in range(N_EPOCHS):\n",
        "            loss = 0\n",
        "            criterion = torch.nn.MSELoss()\n",
        "            #optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "            # make sure we \"zero out\" the loss at each time step\n",
        "            optimizer.zero_grad()\n",
        "            batch_data = next(iter(train_dataloader))\n",
        "            xs, ys = [], []\n",
        "            batch_segments = []\n",
        "            #for idx, batch_data in enumerate(train_dataloader, 0):\n",
        "            for segments, embedding, duration in batch_data:\n",
        "              input_length = len(segments)\n",
        "              encoder_outputs = torch.zeros(input_length, HIDDEN_SIZE,\n",
        "                                              device=DEVICE, requires_grad=False)\n",
        "              if input_length > 0:\n",
        "                input_tensor = tokenize(phone_vocab, segments)\n",
        "                for ei in range(input_length):\n",
        "                  if(type(encoder) is EncoderGloveRNN):\n",
        "                    encoder_output, encoder_hidden = encoder(\n",
        "                      input_tensor[ei], encoder_hidden, torch.Tensor(embedding))\n",
        "                  else:\n",
        "                    encoder_output, encoder_hidden = encoder(\n",
        "                      input_tensor[ei], encoder_hidden)\n",
        "                  #encoder_output, encoder_hidden = encoder(\n",
        "                  #              input_tensor[ei], encoder_hidden)\n",
        "                  encoder_outputs[ei] += encoder_output[0, 0]\n",
        "                  last_hidden_state = encoder_outputs[-1].detach()\n",
        "                  xs.append(last_hidden_state.flatten())\n",
        "                  ys.append(duration)\n",
        "            xs = torch.stack(xs)\n",
        "            ys = torch.Tensor(ys).reshape(-1, 1)\n",
        "            loss = criterion(model(xs), ys)\n",
        "\n",
        "            if i % EVERY == 0:\n",
        "                print(f\"Loss: {loss}\")\n",
        "\n",
        "            loss.backward()\n",
        "            b, m = model.parameters()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        print(f\"Final loss on the training data for encoder {ix} is: {loss}\")  # tensor(0.478)\n",
        "        ## TEST ##\n",
        "        test_criterion = torch.nn.MSELoss()\n",
        "        test_xs, test_ys = [], []\n",
        "        for segments, embedding, duration in batch_data:\n",
        "            input_length = len(segments)\n",
        "            encoder_outputs = torch.zeros(input_length, HIDDEN_SIZE,\n",
        "                                          device=DEVICE, requires_grad=False)\n",
        "            if input_length > 0:\n",
        "                input_tensor = tokenize(phone_vocab, segments)\n",
        "                for ei in range(input_length):\n",
        "                  if(type(encoder) is EncoderGloveRNN):\n",
        "                    encoder_output, encoder_hidden = encoder(\n",
        "                      input_tensor[ei], encoder_hidden, torch.Tensor(embedding))\n",
        "                  else:\n",
        "                    encoder_output, encoder_hidden = encoder(\n",
        "                      input_tensor[ei], encoder_hidden)\n",
        "                    #encoder_output, encoder_hidden = encoder(\n",
        "                    #    input_tensor[ei], encoder_hidden)\n",
        "                    encoder_outputs[ei] += encoder_output[0, 0]\n",
        "                last_hidden_state = encoder_outputs[-1].detach()\n",
        "            test_xs.append(last_hidden_state)\n",
        "            test_ys.append(duration)\n",
        "        test_xs = torch.stack(test_xs)\n",
        "        test_ys = torch.Tensor(test_ys).reshape(-1, 1)\n",
        "        test_loss = test_criterion(model(test_xs), test_ys).detach()\n",
        "        \n",
        "        print(f\"Final loss on the test data for encoder {ix} is: {test_loss}\")  # tensor(0.7235)"
      ],
      "metadata": {
        "id": "f-tlKNtSUmNq"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r67_RRWaVVtx",
        "outputId": "78186385-bc43-473f-ec31-fe2982232b8f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Encoder Decoder\n",
            "Training model\n",
            "Loss: 67.20687866210938\n",
            "Loss: 1.084847092628479\n",
            "Loss: 1.0670005083084106\n",
            "Loss: 0.5969911813735962\n",
            "Loss: 0.6299929618835449\n",
            "Loss: 0.6387811303138733\n",
            "Loss: 0.6426233053207397\n",
            "Loss: 0.5377352833747864\n",
            "Loss: 1.0618247985839844\n",
            "Loss: 0.5671363472938538\n",
            "Final loss on the training data for encoder 0 is: 0.6335338950157166\n",
            "Final loss on the test data for encoder 0 is: 1.0975581407546997\n",
            "Loss: 2.2431862354278564\n",
            "Loss: 1.1846762895584106\n",
            "Loss: 0.9475269317626953\n",
            "Loss: 0.670482873916626\n",
            "Loss: 0.48679137229919434\n",
            "Loss: 0.6842508316040039\n",
            "Loss: 0.6521562337875366\n",
            "Loss: 0.6253276467323303\n",
            "Loss: 0.4932592809200287\n",
            "Loss: 0.6500654220581055\n",
            "Final loss on the training data for encoder 1 is: 0.6114843487739563\n",
            "Final loss on the test data for encoder 1 is: 25.575294494628906\n",
            "Loss: 1.0935804843902588\n",
            "Loss: 0.6774217486381531\n",
            "Loss: 0.5994555950164795\n",
            "Loss: 0.4604150056838989\n",
            "Loss: 0.3727049231529236\n",
            "Loss: 0.5391032099723816\n",
            "Loss: 0.39320316910743713\n",
            "Loss: 0.39382925629615784\n",
            "Loss: 0.4083916246891022\n",
            "Loss: 0.46683356165885925\n",
            "Final loss on the training data for encoder 2 is: 0.41401538252830505\n",
            "Final loss on the test data for encoder 2 is: 0.6045840382575989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hYpYi7kUYfXz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}